{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "unet-3plus-2d-bce-jaccard.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.4.3"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "PNfpi6Q0jXrM",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:10.378891Z",
          "iopub.execute_input": "2021-09-17T08:41:10.379689Z",
          "iopub.status.idle": "2021-09-17T08:41:18.689929Z",
          "shell.execute_reply.started": "2021-09-17T08:41:10.379595Z",
          "shell.execute_reply": "2021-09-17T08:41:18.689132Z"
        },
        "trusted": true,
        "outputId": "0d5019bc-b55f-4921-d2c0-534c7813723c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting keras==2.4.3\n  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras==2.4.3) (1.7.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.4.3) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras==2.4.3) (1.19.5)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.4.3) (5.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras==2.4.3) (1.15.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.4.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.4.1\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:18.693170Z",
          "iopub.execute_input": "2021-09-17T08:41:18.693393Z",
          "iopub.status.idle": "2021-09-17T08:41:25.712336Z",
          "shell.execute_reply.started": "2021-09-17T08:41:18.693365Z",
          "shell.execute_reply": "2021-09-17T08:41:25.711525Z"
        },
        "trusted": true,
        "id": "dQSZQ7xuWAyK",
        "outputId": "2dc2060b-3fed-459a-abf0-ad10f8857beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow==2.4.1 in /opt/conda/lib/python3.7/site-packages (2.4.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.12)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.1.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.3.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.1.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.17.3)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.6.3)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.37.0)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.3.3)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.12.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.19.5)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.15.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.7.4.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.12.1)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.4.0)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.32.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.2.0)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.10.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.25.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.34.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.5)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.6)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n",
        "    add, multiply\n",
        "from keras.layers import concatenate, core, Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.core import Lambda\n",
        "#import keras.backend as K\n",
        "\n",
        "#%tensorflow_version 1.x\n",
        "import os\n",
        "import keras\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "#import keras.backend.tensorflow_backend as K\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import CSVLogger"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:25.715681Z",
          "iopub.execute_input": "2021-09-17T08:41:25.715903Z",
          "iopub.status.idle": "2021-09-17T08:41:30.121017Z",
          "shell.execute_reply.started": "2021-09-17T08:41:25.715875Z",
          "shell.execute_reply": "2021-09-17T08:41:30.120268Z"
        },
        "trusted": true,
        "id": "j0ZIIH4GWAyM",
        "outputId": "b390694d-6b76-4025-b685-63f7f002cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2021-09-17 08:41:26.209545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2"
      ],
      "metadata": {
        "id": "2a8AfzPOjXrQ",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:30.122949Z",
          "iopub.execute_input": "2021-09-17T08:41:30.123286Z",
          "iopub.status.idle": "2021-09-17T08:41:30.907824Z",
          "shell.execute_reply.started": "2021-09-17T08:41:30.123250Z",
          "shell.execute_reply": "2021-09-17T08:41:30.906894Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import cv2\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "BackGround = [255, 255, 255]\n",
        "road = [0, 0, 0]\n",
        "# COLOR_DICT = np.array([BackGround, road])\n",
        "one = [128, 128, 128]\n",
        "two = [128, 0, 0]\n",
        "three = [192, 192, 128]\n",
        "four = [255, 69, 0]\n",
        "five = [128, 64, 128]\n",
        "six = [60, 40, 222]\n",
        "seven = [128, 128, 0]\n",
        "eight = [192, 128, 128]\n",
        "nine = [64, 64, 128]\n",
        "ten = [64, 0, 128]\n",
        "eleven = [64, 64, 0]\n",
        "twelve = [0, 128, 192]\n",
        "COLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n",
        "\n",
        "\n",
        "class data_preprocess:\n",
        "    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n",
        "                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n",
        "                 test_path=None, save_path=None,\n",
        "                 img_rows=256, img_cols=256,\n",
        "                 flag_multi_class=False,\n",
        "                 num_classes = 2):\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.train_path = train_path\n",
        "        self.image_folder = image_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.valid_path = valid_path\n",
        "        self.valid_image_folder = valid_image_folder\n",
        "        self.valid_label_folder = valid_label_folder\n",
        "        self.test_path = test_path\n",
        "        self.save_path = save_path\n",
        "        self.data_gen_args = dict(rotation_range=20,\n",
        "                                  width_shift_range=0.002,\n",
        "                                  shear_range=0.03,\n",
        "                                  zoom_range=0.005,\n",
        "                                  vertical_flip=True,\n",
        "                                  horizontal_flip=True,\n",
        "                                  fill_mode='nearest')\n",
        "        self.image_color_mode = \"rgb\"\n",
        "        self.label_color_mode = \"grayscale\"\n",
        "\n",
        "        self.flag_multi_class = flag_multi_class\n",
        "        self.num_class = num_classes\n",
        "        self.target_size = (256, 256)\n",
        "        self.img_type = 'png'\n",
        "\n",
        "    def adjustData(self, img, label):\n",
        "        if (self.flag_multi_class):\n",
        "            img = img / 255.\n",
        "            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n",
        "            new_label = np.zeros(label.shape + (self.num_class,))\n",
        "            for i in range(self.num_class):\n",
        "                new_label[label == i, i] = 1\n",
        "            label = new_label\n",
        "        elif (np.max(img) > 1):\n",
        "            #img = img / 255.\n",
        "            #label = label / 255.\n",
        "            #label[label >= 0.5] = 1\n",
        "            #label[label < 0.5] = 0\n",
        "            img2 =np.asarray(img)\n",
        "            label2 =np.asarray(label)\n",
        "            img2 =img2.astype('float32')\n",
        "            label2 =label2.astype('float32')\n",
        "            img2 /= 255.0\n",
        "            label2 /= 255.0\n",
        "            label2[label2 >= 0.5] = 1\n",
        "            label2[label2 < 0.5] = 0\n",
        "        return (img2, label2)\n",
        "\n",
        "    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n",
        "                       save_to_dir=None, seed=7):\n",
        "        '''\n",
        "        can generate image and label at the same time\n",
        "        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n",
        "        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
        "        '''\n",
        "        image_datagen = ImageDataGenerator(**self.data_gen_args)\n",
        "        label_datagen = ImageDataGenerator(**self.data_gen_args)\n",
        "        image_generator = image_datagen.flow_from_directory(\n",
        "            self.train_path,\n",
        "            classes=[self.image_folder],\n",
        "            class_mode=None,\n",
        "            color_mode=self.image_color_mode,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=batch_size,\n",
        "            save_to_dir=save_to_dir,\n",
        "            save_prefix=image_save_prefix,\n",
        "            seed=seed)\n",
        "        label_generator = label_datagen.flow_from_directory(\n",
        "            self.train_path,\n",
        "            classes=[self.label_folder],\n",
        "            class_mode=None,\n",
        "            color_mode=self.label_color_mode,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=batch_size,\n",
        "            save_to_dir=save_to_dir,\n",
        "            save_prefix=label_save_prefix,\n",
        "            seed=seed)\n",
        "        train_generator = zip(image_generator, label_generator)\n",
        "        for (img, label) in train_generator:\n",
        "            img, label = self.adjustData(img, label)\n",
        "            yield (img, label)\n",
        "\n",
        "    def testGenerator(self):\n",
        "        filenames = os.listdir(self.test_path)\n",
        "        for filename in filenames:\n",
        "            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n",
        "            img = img / 255.\n",
        "            img = trans.resize(img, self.target_size, mode='constant')\n",
        "            img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n",
        "            img = np.reshape(img, (1,) + img.shape)\n",
        "            yield img\n",
        "\n",
        "    def validLoad(self, batch_size,seed=7):\n",
        "        image_datagen = ImageDataGenerator(rescale=0)\n",
        "        label_datagen = ImageDataGenerator(rescale=0)\n",
        "        image_generator = image_datagen.flow_from_directory(\n",
        "            self.valid_path,\n",
        "            classes=[self.valid_image_folder],\n",
        "            class_mode=None,\n",
        "            color_mode=self.image_color_mode,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=batch_size,\n",
        "            seed=seed)\n",
        "        label_generator = label_datagen.flow_from_directory(\n",
        "            self.valid_path,\n",
        "            classes=[self.valid_label_folder],\n",
        "            class_mode=None,\n",
        "            color_mode=self.label_color_mode,\n",
        "            target_size=self.target_size,\n",
        "            batch_size=batch_size,\n",
        "            seed=seed)\n",
        "        train_generator = zip(image_generator, label_generator)\n",
        "        for (img, label) in train_generator:\n",
        "            img, label = self.adjustData(img, label)\n",
        "            yield (img, label)\n",
        "        # return imgs,labels\n",
        "\n",
        "    def saveResult(self, npyfile, size, name,threshold=80):\n",
        "        for i, item in enumerate(npyfile):\n",
        "            img = item\n",
        "            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "            if self.flag_multi_class:\n",
        "                for row in range(len(img)):\n",
        "                    for col in range(len(img[row])):\n",
        "                        num = np.argmax(img[row][col])\n",
        "                        img_std[row][col] = COLOR_DICT[num]\n",
        "            else:\n",
        "                for k in range(len(img)):\n",
        "                    for j in range(len(img[k])):\n",
        "                        num = img[k][j]\n",
        "                        if num < (threshold/255.0):\n",
        "                            img_std[k][j] = road\n",
        "                        else:\n",
        "                            img_std[k][j] = BackGround\n",
        "            img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n",
        "            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)"
      ],
      "metadata": {
        "id": "iTf8lIwLjXrS",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:30.909321Z",
          "iopub.execute_input": "2021-09-17T08:41:30.909597Z",
          "iopub.status.idle": "2021-09-17T08:41:31.409755Z",
          "shell.execute_reply.started": "2021-09-17T08:41:30.909561Z",
          "shell.execute_reply": "2021-09-17T08:41:31.409049Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####  Metrics\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "def iou_coeff(y_true, y_pred):\n",
        "    smooth=1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n",
        "    mvalue=(intersection+smooth)/(union+smooth)\n",
        "    return mvalue\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "def ACL5(y_true, y_pred): \n",
        "\n",
        "\t#y_pred = K.cast(y_pred, dtype = 'float64')\n",
        "\n",
        "\tprint(K.int_shape(y_pred))\n",
        "\n",
        "\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n",
        "\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n",
        "\n",
        "\tdelta_x = x[:,1:,:-2,:]**2\n",
        "\tdelta_y = y[:,:-2,1:,:]**2\n",
        "\tdelta_u = K.abs(delta_x + delta_y) \n",
        "\n",
        "\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n",
        "\tw = 1####\n",
        "\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n",
        "\n",
        "\n",
        "\tC_1 = np.ones((256, 256))\n",
        "\tC_2 = np.zeros((256, 256))\n",
        "\n",
        "\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n",
        "\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n",
        "\n",
        "\tlambdaP = 5 # lambda parameter could be various.\n",
        "\t\n",
        "\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n",
        "\n",
        "\treturn loss\n",
        "def ACL5_mod(y_true, y_pred): \n",
        "\n",
        "\t#y_pred = K.cast(y_pred, dtype = 'float64')\n",
        "\n",
        "\tprint(K.int_shape(y_pred))\n",
        "\n",
        "\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n",
        "\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n",
        "\n",
        "\tdelta_x = x[:,1:,:-2,:]**2\n",
        "\tdelta_y = y[:,:-2,1:,:]**2\n",
        "\tdelta_u = K.abs(delta_x + delta_y) \n",
        "\n",
        "\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n",
        "\tw = 1####\n",
        "\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n",
        "\n",
        "\n",
        "\tC_1 = np.ones((256, 256))\n",
        "\tC_2 = np.zeros((256, 256))\n",
        "\n",
        "\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n",
        "\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n",
        "\n",
        "\tlambdaP = 5 # lambda parameter could be various.\n",
        "\t\n",
        "\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n",
        "\n",
        "\treturn loss"
      ],
      "metadata": {
        "id": "VVdzJaSWjXra",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:31.411033Z",
          "iopub.execute_input": "2021-09-17T08:41:31.411279Z",
          "iopub.status.idle": "2021-09-17T08:41:31.435199Z",
          "shell.execute_reply.started": "2021-09-17T08:41:31.411249Z",
          "shell.execute_reply": "2021-09-17T08:41:31.434167Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iwu9rjDFjXrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.25\n",
        "alpha = 0.25\n",
        "gamma = 2\n",
        "epsilon = 1e-5\n",
        "smooth = 1\n",
        "\n",
        "def tversky_index( y_true, y_pred):\n",
        "    y_true_pos = K.flatten(y_true)\n",
        "    y_pred_pos = K.flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
        "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
        "    alpha = 0.7\n",
        "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n",
        "                1 - alpha) * false_pos + smooth)\n",
        "\n",
        "def tversky_loss( y_true, y_pred):\n",
        "    return 1 - tversky_index(y_true, y_pred)\n",
        "\n",
        "def focal_tversky( y_true, y_pred):\n",
        "    pt_1 = tversky_index(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.pow((1 - pt_1), gamma)\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
        "  return dice\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dsc(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-YuW43lejXrd",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:31.436685Z",
          "iopub.execute_input": "2021-09-17T08:41:31.437071Z",
          "iopub.status.idle": "2021-09-17T08:41:31.451190Z",
          "shell.execute_reply.started": "2021-09-17T08:41:31.437036Z",
          "shell.execute_reply": "2021-09-17T08:41:31.450217Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env SM_FRAMEWORK=tf.keras"
      ],
      "metadata": {
        "id": "kr5nP8ItjXre",
        "outputId": "48e77d7c-c144-4f17-e726-e4b36dce7171",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:31.452491Z",
          "iopub.execute_input": "2021-09-17T08:41:31.453079Z",
          "iopub.status.idle": "2021-09-17T08:41:31.464199Z",
          "shell.execute_reply.started": "2021-09-17T08:41:31.453046Z",
          "shell.execute_reply": "2021-09-17T08:41:31.463292Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "env: SM_FRAMEWORK=tf.keras\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models\n",
        "import segmentation_models\n",
        "from segmentation_models.losses import bce_jaccard_loss"
      ],
      "metadata": {
        "id": "KMTFbTKtjXrf",
        "outputId": "2bd1e0fe-2f4f-479d-e8e6-b255673c5ad3",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:31.465499Z",
          "iopub.execute_input": "2021-09-17T08:41:31.465798Z",
          "iopub.status.idle": "2021-09-17T08:41:39.389017Z",
          "shell.execute_reply.started": "2021-09-17T08:41:31.465763Z",
          "shell.execute_reply": "2021-09-17T08:41:39.388231Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting segmentation_models\n  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\nCollecting efficientnet==1.0.0\n  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\nCollecting image-classifiers==1.0.0\n  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 1.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.3)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (8.3.1)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.8.30)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.7.1)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.3)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (5.0.9)\nInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\nSuccessfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nSegmentation Models: using `tf.keras` framework.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ACL5_bce_jaccard_loss(y_true, y_pred):\n",
        "    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def focal_tversky_bce_jaccard_loss(y_true, y_pred):\n",
        "    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "IVcpwi4hjXrg",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.392503Z",
          "iopub.execute_input": "2021-09-17T08:41:39.392715Z",
          "iopub.status.idle": "2021-09-17T08:41:39.400012Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.392689Z",
          "shell.execute_reply": "2021-09-17T08:41:39.399298Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.layers import Conv2D , BatchNormalization , Activation , MaxPool2D , Input , Dropout , ZeroPadding2D , Conv2DTranspose , Concatenate\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.applications import InceptionResNetV2"
      ],
      "metadata": {
        "id": "eDJ9CtzQjXrh",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.401204Z",
          "iopub.execute_input": "2021-09-17T08:41:39.401502Z",
          "iopub.status.idle": "2021-09-17T08:41:39.408589Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.401432Z",
          "shell.execute_reply": "2021-09-17T08:41:39.407823Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unet3Plus_ACL5"
      ],
      "metadata": {
        "id": "ccYNxPZwjXri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kFchkooujXrk",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "#path to images\n",
        "train_path = \"../input/training/training\"\n",
        "image_folder = \"images\"\n",
        "label_folder = \"label\"\n",
        "valid_path =  \"../input/validation/Validation\"\n",
        "valid_image_folder =\"images\"\n",
        "valid_label_folder = \"label\"\n",
        "log_filepath = './log'\n",
        "flag_multi_class = False\n",
        "num_classes = 2\n",
        "dp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n",
        "                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n",
        "                     flag_multi_class=flag_multi_class,\n",
        "                     num_classes=num_classes)\n",
        "\n",
        "train_data = dp.trainGenerator(batch_size=2)\n",
        "valid_data = dp.validLoad(batch_size=1)\n",
        "test_data = dp.testGenerator()\n",
        "lrate = 7.00E-05 \n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Multiply, AveragePooling2D, UpSampling2D\n",
        "\n"
      ],
      "metadata": {
        "id": "z73b0ddqjXrl",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.409676Z",
          "iopub.execute_input": "2021-09-17T08:41:39.410504Z",
          "iopub.status.idle": "2021-09-17T08:41:39.420163Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.410470Z",
          "shell.execute_reply": "2021-09-17T08:41:39.419427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "xl1qWq5VWAyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow.compat.v1 import image\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D, Conv2DTranspose, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, concatenate, multiply, add\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU, PReLU, ELU, Softmax\n",
        "\n",
        "def decode_layer(X, channel, pool_size, unpool, kernel_size=3, \n",
        "                 activation='ReLU', batch_norm=False, name='decode'):\n",
        "    '''\n",
        "    An overall decode layer, based on either upsampling or trans conv.\n",
        "    \n",
        "    decode_layer(X, channel, pool_size, unpool, kernel_size=3,\n",
        "                 activation='ReLU', batch_norm=False, name='decode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the decoding factor.\n",
        "        channel: (for trans conv only) number of convolution filters.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.           \n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    * The defaut: `kernel_size=3`, is suitable for `pool_size=2`.\n",
        "    \n",
        "    '''\n",
        "    # parsers\n",
        "    if unpool is False:\n",
        "        # trans conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    elif unpool == 'nearest':\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'nearest'\n",
        "    \n",
        "    elif (unpool is True) or (unpool == 'bilinear'):\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'bilinear'\n",
        "    \n",
        "    else:\n",
        "        raise ValueError('Invalid unpool keyword')\n",
        "        \n",
        "    if unpool:\n",
        "        X = UpSampling2D(size=(pool_size, pool_size), interpolation=interp, name='{}_unpool'.format(name))(X)\n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "            \n",
        "        X = Conv2DTranspose(channel, kernel_size, strides=(pool_size, pool_size), \n",
        "                            padding='same', name='{}_trans_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "        X = Dropout(0.2)(X)\n",
        "    return X\n",
        "\n",
        "def encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode'):\n",
        "    '''\n",
        "    An overall encode layer, based on one of the:\n",
        "    (1) max-pooling, (2) average-pooling, (3) strided conv2d.\n",
        "    \n",
        "    encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the encoding factor.\n",
        "        channel: (for strided conv only) number of convolution filters.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    # parsers\n",
        "    if (pool in [False, True, 'max', 'ave']) is not True:\n",
        "        raise ValueError('Invalid pool keyword')\n",
        "        \n",
        "    # maxpooling2d as default\n",
        "    if pool is True:\n",
        "        pool = 'max'\n",
        "        \n",
        "    elif pool is False:\n",
        "        # stride conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    if pool == 'max':\n",
        "        X = MaxPooling2D(pool_size=(pool_size, pool_size), name='{}_maxpool'.format(name))(X)\n",
        "        \n",
        "    elif pool == 'ave':\n",
        "        X = AveragePooling2D(pool_size=(pool_size, pool_size), name='{}_avepool'.format(name))(X)\n",
        "        \n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "        \n",
        "        # linear convolution with strides\n",
        "        X = Conv2D(channel, kernel_size, strides=(pool_size, pool_size), \n",
        "                   padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "        X = Dropout(0.2)(X)\n",
        "    return X\n",
        "\n",
        "def attention_gate(X, g, channel,  \n",
        "                   activation='ReLU', \n",
        "                   attention='add', name='att'):\n",
        "    '''\n",
        "    Self-attention gate modified from Oktay et al. 2018.\n",
        "    \n",
        "    attention_gate(X, g, channel,  activation='ReLU', attention='add', name='att')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor, i.e., key and value.\n",
        "        g: gated tensor, i.e., query.\n",
        "        channel: number of intermediate channel.\n",
        "                 Oktay et al. (2018) did not specify (denoted as F_int).\n",
        "                 intermediate channel is expected to be smaller than the input channel.\n",
        "        activation: a nonlinear attnetion activation.\n",
        "                    The `sigma_1` in Oktay et al. 2018. Default is 'ReLU'.\n",
        "        attention: 'add' for additive attention; 'multiply' for multiplicative attention.\n",
        "                   Oktay et al. 2018 applied additive attention.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X_att: output tensor.\n",
        "    \n",
        "    '''\n",
        "    activation_func = eval(activation)\n",
        "    attention_func = eval(attention)\n",
        "    \n",
        "    # mapping the input tensor to the intermediate channel\n",
        "    theta_att = Conv2D(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\n",
        "    \n",
        "    # mapping the gate tensor\n",
        "    phi_g = Conv2D(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\n",
        "    \n",
        "    # ----- attention learning ----- #\n",
        "    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\n",
        "    \n",
        "    # nonlinear activation\n",
        "    f = activation_func(name='{}_activation'.format(name))(query)\n",
        "    \n",
        "    # linear transformation\n",
        "    psi_f = Conv2D(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\n",
        "    # ------------------------------ #\n",
        "    \n",
        "    # sigmoid activation as attention coefficients\n",
        "    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\n",
        "    \n",
        "    # multiplicative attention masking\n",
        "    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\n",
        "    \n",
        "    return X_att\n",
        "\n",
        "def CONV_stack(X, channel, kernel_size=3, stack_num=2, \n",
        "               dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack'):\n",
        "    '''\n",
        "    Stacked convolutional layers:\n",
        "    (Convolutional layer --> batch normalization --> Activation)*stack_num\n",
        "    \n",
        "    CONV_stack(X, channel, kernel_size=3, stack_num=2, dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack')\n",
        "    \n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked Conv2D-BN-Activation layers.\n",
        "        dilation_rate: optional dilated convolution kernel.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    # stacking Convolutional layers\n",
        "    for i in range(stack_num):\n",
        "        \n",
        "        activation_func = eval(activation)\n",
        "        \n",
        "        # linear convolution\n",
        "        X = Conv2D(channel, kernel_size, padding='same', use_bias=bias_flag, \n",
        "                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\n",
        "        \n",
        "        # activation\n",
        "        activation_func = eval(activation)\n",
        "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "\n",
        "def Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv'):\n",
        "    '''\n",
        "    Stacked convolutional layers with residual path.\n",
        "     \n",
        "    Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv')\n",
        "     \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_skip: the tensor that does go into the residual path \n",
        "                can be a copy of X (e.g., the identity block of ResNet).\n",
        "        channel: number of convolution filters.\n",
        "        res_num: number of convolutional layers within the residual path.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''  \n",
        "    X = CONV_stack(X, channel, kernel_size=3, stack_num=res_num, dilation_rate=1, \n",
        "                   activation=activation, batch_norm=batch_norm, name=name)\n",
        "\n",
        "    X = add([X_skip, X], name='{}_add'.format(name))\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    X = activation_func(name='{}_add_activation'.format(name))(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv'):\n",
        "    '''\n",
        "    Depthwise separable convolution with (optional) dilated convolution kernel and batch normalization.\n",
        "    \n",
        "    Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked depthwise-pointwise layers.\n",
        "        dilation_rate: optional dilated convolution kernel.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    for i in range(stack_num):\n",
        "        X = DepthwiseConv2D(kernel_size, dilation_rate=dilation_rate, padding='same', \n",
        "                            use_bias=bias_flag, name='{}_{}_depthwise'.format(name, i))(X)\n",
        "        \n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(name='{}_{}_depthwise_BN'.format(name, i))(X)\n",
        "\n",
        "        X = activation_func(name='{}_{}_depthwise_activation'.format(name, i))(X)\n",
        "\n",
        "        X = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_{}_pointwise'.format(name, i))(X)\n",
        "        \n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(name='{}_{}_pointwise_BN'.format(name, i))(X)\n",
        "\n",
        "        X = activation_func(name='{}_{}_pointwise_activation'.format(name, i))(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp'):\n",
        "    '''\n",
        "    Atrous Spatial Pyramid Pooling (ASPP).\n",
        "    \n",
        "    ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp')\n",
        "    \n",
        "    ----------\n",
        "    Wang, Y., Liang, B., Ding, M. and Li, J., 2019. Dense semantic labeling \n",
        "    with atrous spatial pyramid pooling and decoder for high-resolution remote \n",
        "    sensing imagery. Remote Sensing, 11(1), p.20.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    * dilation rates are fixed to `[6, 9, 12]`.\n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    bias_flag = not batch_norm\n",
        "\n",
        "    shape_before = X.get_shape().as_list()\n",
        "    b4 = GlobalAveragePooling2D(name='{}_avepool_b4'.format(name))(X)\n",
        "    \n",
        "    b4 = expand_dims(expand_dims(b4, 1), 1, name='{}_expdim_b4'.format(name))\n",
        "    \n",
        "    b4 = Conv2D(channel, 1, padding='same', use_bias=bias_flag, name='{}_conv_b4'.format(name))(b4)\n",
        "    \n",
        "    if batch_norm:\n",
        "        b4 = BatchNormalization(name='{}_conv_b4_BN'.format(name))(b4)\n",
        "        \n",
        "    b4 = activation_func(name='{}_conv_b4_activation'.format(name))(b4)\n",
        "    \n",
        "    # <----- tensorflow v1 resize.\n",
        "    b4 = Lambda(lambda X: image.resize(X, shape_before[1:3], method='bilinear', align_corners=True), \n",
        "                name='{}_resize_b4'.format(name))(b4)\n",
        "    \n",
        "    b0 = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_conv_b0'.format(name))(X)\n",
        "\n",
        "    if batch_norm:\n",
        "        b0 = BatchNormalization(name='{}_conv_b0_BN'.format(name))(b0)\n",
        "        \n",
        "    b0 = activation_func(name='{}_conv_b0_activation'.format(name))(b0)\n",
        "    \n",
        "    # dilation rates are fixed to `[6, 9, 12]`.\n",
        "    b_r6 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=6, batch_norm=True, name='{}_sepconv_r6'.format(name))\n",
        "    b_r9 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=9, batch_norm=True, name='{}_sepconv_r9'.format(name))\n",
        "    b_r12 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=12, batch_norm=True, name='{}_sepconv_r12'.format(name))\n",
        "    \n",
        "    return concatenate([b4, b0, b_r6, b_r9, b_r12])\n",
        "\n",
        "def CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output'):\n",
        "    '''\n",
        "    Convolutional layer with output activation.\n",
        "    \n",
        "    CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        n_labels: number of classification label(s).\n",
        "        kernel_size: size of 2-d convolution kernels. Default is 1-by-1.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                    Default option is 'Softmax'.\n",
        "                    if None is received, then linear activation is applied.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    X = Conv2D(n_labels, kernel_size, padding='same', use_bias=True, name=name)(X)\n",
        "    \n",
        "    if activation:\n",
        "        \n",
        "        if activation == 'Sigmoid':\n",
        "            X = Activation('sigmoid', name='{}_activation'.format(name))(X)\n",
        "            \n",
        "        else:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "            \n",
        "    return X"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.422615Z",
          "iopub.execute_input": "2021-09-17T08:41:39.422817Z",
          "iopub.status.idle": "2021-09-17T08:41:39.522491Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.422793Z",
          "shell.execute_reply": "2021-09-17T08:41:39.521831Z"
        },
        "trusted": true,
        "id": "YQd1HL3kWAyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "layer_cadidates = {\n",
        "    'VGG16': ('block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3'),\n",
        "    'VGG19': ('block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4'),\n",
        "    'ResNet50': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out'),\n",
        "    'ResNet101': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block23_out', 'conv5_block3_out'),\n",
        "    'ResNet152': ('conv1_relu', 'conv2_block3_out', 'conv3_block8_out', 'conv4_block36_out', 'conv5_block3_out'),\n",
        "    'ResNet50V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block6_1_relu', 'post_relu'),\n",
        "    'ResNet101V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block23_1_relu', 'post_relu'),\n",
        "    'ResNet152V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block8_1_relu', 'conv4_block36_1_relu', 'post_relu'),\n",
        "    'DenseNet121': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'DenseNet169': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'DenseNet201': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'EfficientNetB0': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB1': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB2': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB3': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB4': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB5': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB6': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB7': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),}\n",
        "\n",
        "def bach_norm_checker(backbone_name, batch_norm):\n",
        "    '''batch norm checker'''\n",
        "    if 'VGG' in backbone_name:\n",
        "        batch_norm_backbone = False\n",
        "    else:\n",
        "        batch_norm_backbone = True\n",
        "        \n",
        "    if batch_norm_backbone != batch_norm:       \n",
        "        if batch_norm_backbone:    \n",
        "            param_mismatch = \"\\n\\nBackbone {} uses batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n",
        "        else:\n",
        "            param_mismatch = \"\\n\\nBackbone {} does not use batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n",
        "            \n",
        "        warnings.warn(param_mismatch);\n",
        "        \n",
        "def backbone_zoo(backbone_name, weights, input_tensor, depth, freeze_backbone, freeze_batch_norm):\n",
        "    '''\n",
        "    Configuring a user specified encoder model based on the `tensorflow.keras.applications`\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        backbone_name: the bakcbone model name. Expected as one of the `tensorflow.keras.applications` class.\n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0,7]\n",
        "                       \n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        input_tensor: the input tensor \n",
        "        depth: number of encoded feature maps. \n",
        "               If four dwonsampling levels are needed, then depth=4.\n",
        "        \n",
        "        freeze_backbone: True for a frozen backbone\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras backbone model.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    cadidate = layer_cadidates[backbone_name]\n",
        "    \n",
        "    # ----- #\n",
        "    # depth checking\n",
        "    depth_max = len(cadidate)\n",
        "    if depth > depth_max:\n",
        "        depth = depth_max\n",
        "    # ----- #\n",
        "    \n",
        "    backbone_func = eval(backbone_name)\n",
        "    backbone_ = backbone_func(include_top=False, weights=weights, input_tensor=input_tensor, pooling=None,)\n",
        "    \n",
        "    X_skip = []\n",
        "    \n",
        "    for i in range(depth):\n",
        "        X_skip.append(backbone_.get_layer(cadidate[i]).output)\n",
        "        \n",
        "    model = Model(inputs=[input_tensor,], outputs=X_skip, name='{}_backbone'.format(backbone_name))\n",
        "    \n",
        "    if freeze_backbone:\n",
        "        \n",
        "        model = freeze_model(model, freeze_batch_norm=freeze_batch_norm)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.523687Z",
          "iopub.execute_input": "2021-09-17T08:41:39.523923Z",
          "iopub.status.idle": "2021-09-17T08:41:39.542642Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.523895Z",
          "shell.execute_reply": "2021-09-17T08:41:39.541817Z"
        },
        "trusted": true,
        "id": "VkK0Kd5lWAyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bYl_NIVlWAyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n",
        "              pool=True, batch_norm=False, name='left0'):\n",
        "    '''\n",
        "    The encoder block of U-net.\n",
        "    \n",
        "    UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n",
        "              pool=True, batch_norm=False, name='left0')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of convolutional layers.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    pool_size = 2\n",
        "    \n",
        "    X = encode_layer(X, channel, pool_size, pool, activation=activation, \n",
        "                     batch_norm=batch_norm, name='{}_encode'.format(name))\n",
        "\n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv'.format(name))\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def UNET_right(X, X_list, channel, kernel_size=3, \n",
        "               stack_num=2, activation='ReLU',\n",
        "               unpool=True, batch_norm=False, concat=True, name='right0'):\n",
        "    \n",
        "    '''\n",
        "    The decoder block of U-net.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_list: a list of other tensors that connected to the input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of convolutional layers.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        concat: True for concatenating the corresponded X_list elements.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    pool_size = 2\n",
        "    \n",
        "    X = decode_layer(X, channel, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
        "    \n",
        "    # linear convolutional layers before concatenation\n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=1, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv_before_concat'.format(name))\n",
        "    if concat:\n",
        "        # <--- *stacked convolutional can be applied here\n",
        "        X = concatenate([X,]+X_list, axis=3, name=name+'_concat')\n",
        "    \n",
        "    # Stacked convolutions after concatenation \n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name=name+'_conv_after_concat')\n",
        "    \n",
        "    return X"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.544123Z",
          "iopub.execute_input": "2021-09-17T08:41:39.544414Z",
          "iopub.status.idle": "2021-09-17T08:41:39.557281Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.544379Z",
          "shell.execute_reply": "2021-09-17T08:41:39.556563Z"
        },
        "trusted": true,
        "id": "cG9KKMb0WAyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Unet 3+ Plus\n",
        "\n",
        "def unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n",
        "                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n",
        "    '''\n",
        "    The base of UNET 3+ with an optional ImagNet-trained backbone.\n",
        "    \n",
        "    unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n",
        "                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n",
        "                  \n",
        "    ----------\n",
        "    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n",
        "    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n",
        "    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n",
        "    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.        \n",
        "        filter_num_down: a list that defines the number of filters for each \n",
        "                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n",
        "                         the network depth is expected as `len(filter_num_down)`\n",
        "        filter_num_skip: a list that defines the number of filters after each \n",
        "                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n",
        "                         i.e., the bottom level is not included.\n",
        "                         * Huang et al. (2020) applied the same numbers for all levels. \n",
        "                           e.g., `[64, 64, 64, 64]`.\n",
        "        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.          \n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., ReLU                \n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.     \n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.   \n",
        "    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n",
        "    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n",
        "    \n",
        "    Output\n",
        "    ----------\n",
        "        A list of tensors with the first/second/third tensor obtained from \n",
        "        the deepest/second deepest/third deepest upsampling block, etc.\n",
        "        * The feature map sizes of these tensors are different, \n",
        "          with the first tensor has the smallest size. \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    depth_ = len(filter_num_down)\n",
        "\n",
        "    X_encoder = []\n",
        "    X_decoder = []\n",
        "\n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "        X = ASPP_conv(X, filter_num_down[0], activation='ReLU', batch_norm=True , name='{}_ASPP_conv'.format(name))\n",
        "        # stacked conv2d before downsampling\n",
        "        #X = CONV_stack(X, filter_num_down[0], kernel_size=3, stack_num=stack_num_down, \n",
        "        #              activation=activation, batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "        X_encoder.append(X)\n",
        "\n",
        "        # downsampling levels\n",
        "        for i, f in enumerate(filter_num_down[1:]):\n",
        "\n",
        "            # UNET-like downsampling\n",
        "            X = UNET_left(X, f, kernel_size=3, stack_num=stack_num_down, activation=activation, \n",
        "                          pool=pool, batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))\n",
        "            X_encoder.append(X)\n",
        "\n",
        "    else:\n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_encoder = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_encoder)\n",
        "\n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_encoder = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_encoder) + 1\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor  \n",
        "            X = X_encoder[-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num_down[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_encoder.append(X)\n",
        "\n",
        "\n",
        "    # treat the last encoded tensor as the first decoded tensor\n",
        "    X_decoder.append(X_encoder[-1])\n",
        "\n",
        "    # upsampling levels\n",
        "    X_encoder = X_encoder[::-1]\n",
        "\n",
        "    depth_decode = len(X_encoder)-1\n",
        "\n",
        "    # loop over upsampling levels\n",
        "    for i in range(depth_decode):\n",
        "\n",
        "        f = filter_num_skip[i]\n",
        "\n",
        "        # collecting tensors for layer fusion\n",
        "        X_fscale = []\n",
        "\n",
        "        # for each upsampling level, loop over all available downsampling levels (similar to the unet++)\n",
        "        for lev in range(depth_decode):\n",
        "\n",
        "            # counting scale difference between the current down- and upsampling levels\n",
        "            pool_scale = lev-i-1 # -1 for python indexing\n",
        "\n",
        "            # deeper tensors are obtained from **decoder** outputs\n",
        "            if pool_scale < 0:\n",
        "                pool_size = 2**(-1*pool_scale)\n",
        "                \n",
        "                X = decode_layer(X_decoder[lev], f, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_up_{}_en{}'.format(name, i, lev))\n",
        "\n",
        "            # unet skip connection (identity mapping)    \n",
        "            elif pool_scale == 0:\n",
        "\n",
        "                X = X_encoder[lev]\n",
        "\n",
        "            # shallower tensors are obtained from **encoder** outputs\n",
        "            else:\n",
        "                pool_size = 2**(pool_scale)\n",
        "                \n",
        "                X = encode_layer(X_encoder[lev], f, pool_size, pool, activation=activation, \n",
        "                                 batch_norm=batch_norm, name='{}_down_{}_en{}'.format(name, i, lev))\n",
        "\n",
        "            # a conv layer after feature map scale change\n",
        "            #X = CONV_stack(X, f, kernel_size=3, stack_num=1, \n",
        "            #               activation=activation, batch_norm=batch_norm, name='{}_down_from{}_to{}'.format(name, i, lev))\n",
        "            X = ASPP_conv(X, f , activation='ReLU', batch_norm=True , name='{}_down_from{}_to{}'.format(name, i, lev))\n",
        "            X_fscale.append(X)  \n",
        "\n",
        "        # layer fusion at the end of each level\n",
        "        # stacked conv layers after concat. BatchNormalization is fixed to True\n",
        "\n",
        "        X = concatenate(X_fscale, axis=-1, name='{}_concat_{}'.format(name, i))\n",
        "        X = CONV_stack(X, filter_num_aggregate, kernel_size=3, stack_num=stack_num_up, \n",
        "                       activation=activation, batch_norm=True, name='{}_fusion_conv_{}'.format(name, i))\n",
        "        X_decoder.append(X)\n",
        "\n",
        "    # if tensors for concatenation is not enough\n",
        "    # then use upsampling without concatenation \n",
        "    if depth_decode < depth_-1:\n",
        "        for i in range(depth_-depth_decode-1):\n",
        "            i_real = i + depth_decode\n",
        "            X = UNET_right(X, None, filter_num_aggregate, stack_num=stack_num_up, activation=activation, \n",
        "                           unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_plain_up{}'.format(name, i_real))\n",
        "            X_decoder.append(X)\n",
        "        \n",
        "    # return decoder outputs\n",
        "    return X_decoder\n",
        "\n",
        "def unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='sigmoid',\n",
        "                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n",
        "    \n",
        "    '''\n",
        "    UNET 3+ with an optional ImageNet-trained backbone.\n",
        "    \n",
        "    unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
        "                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n",
        "                  \n",
        "    ----------\n",
        "    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n",
        "    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n",
        "    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n",
        "    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n",
        "        filter_num_down: a list that defines the number of filters for each \n",
        "                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n",
        "                         the network depth is expected as `len(filter_num_down)`\n",
        "        filter_num_skip: a list that defines the number of filters after each \n",
        "                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n",
        "                         i.e., the bottom level is not included.\n",
        "                         * Huang et al. (2020) applied the same numbers for all levels. \n",
        "                           e.g., `[64, 64, 64, 64]`.\n",
        "        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'\n",
        "        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                           Default option is 'Softmax'.\n",
        "                           if None is received, then linear activation is applied.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.   \n",
        "        deep_supervision: True for a model that supports deep supervision. Details see Huang et al. (2020).\n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.   \n",
        "        \n",
        "    * The Classification-guided Module (CGM) is not implemented. \n",
        "      See https://github.com/yingkaisha/keras-unet-collection/tree/main/examples for a relevant example.\n",
        "    * Automated mode is applied for determining `filter_num_skip`, `filter_num_aggregate`.\n",
        "    * The default output activation is sigmoid, consistent with Huang et al. (2020).\n",
        "    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n",
        "    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n",
        "    \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras model.\n",
        "    \n",
        "    '''\n",
        "\n",
        "    depth_ = len(filter_num_down)\n",
        "    \n",
        "    verbose = False\n",
        "    \n",
        "    if filter_num_skip == 'auto':\n",
        "        verbose = True\n",
        "        filter_num_skip = [filter_num_down[0] for num in range(depth_-1)]\n",
        "        \n",
        "    if filter_num_aggregate == 'auto':\n",
        "        verbose = True\n",
        "        filter_num_aggregate = int(depth_*filter_num_down[0])\n",
        "        \n",
        "    if verbose:\n",
        "        print('Automated hyper-parameter determination is applied with the following details:\\n----------')\n",
        "        print('\\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = {}'.format(filter_num_skip))\n",
        "        print('\\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = {}'.format(filter_num_aggregate))    \n",
        "    \n",
        "    if backbone is not None:\n",
        "        bach_norm_checker(backbone, batch_norm)\n",
        "    \n",
        "    X_encoder = []\n",
        "    X_decoder = []\n",
        "\n",
        "\n",
        "    IN = Input(input_size)\n",
        "\n",
        "    X_decoder = unet_3plus_2d_base(IN, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                                   stack_num_down=stack_num_down, stack_num_up=stack_num_up, activation=activation, \n",
        "                                   batch_norm=batch_norm, pool=pool, unpool=unpool, \n",
        "                                   backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n",
        "                                   freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "    X_decoder = X_decoder[::-1]\n",
        "\n",
        "    if deep_supervision:\n",
        "        \n",
        "        # ----- frozen backbone issue checker ----- #\n",
        "        if ('{}_backbone_'.format(backbone) in X_decoder[0].name) and freeze_backbone:\n",
        "            \n",
        "            backbone_warn = '\\n\\nThe deepest UNET 3+ deep supervision branch directly connects to a frozen backbone.\\nTesting your configurations on `keras_unet_collection.base.unet_plus_2d_base` is recommended.'\n",
        "            warnings.warn(backbone_warn);\n",
        "        # ----------------------------------------- #\n",
        "        \n",
        "        OUT_stack = []\n",
        "        L_out = len(X_decoder)\n",
        "        \n",
        "        print('----------\\ndeep_supervision = True\\nnames of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\\n\"final\" is the final output layer):\\n')\n",
        "        \n",
        "        # conv2d --> upsampling --> output activation.\n",
        "        # index 0 is final output \n",
        "        for i in range(1, L_out):\n",
        "            \n",
        "            pool_size = 2**(i)\n",
        "            \n",
        "            X = Conv2D(n_labels, 3, padding='same', name='{}_output_conv_{}'.format(name, i-1))(X_decoder[i])\n",
        "            \n",
        "            X = decode_layer(X, n_labels, pool_size, unpool, \n",
        "                             activation=None, batch_norm=False, name='{}_output_sup{}'.format(name, i-1))\n",
        "            \n",
        "            if output_activation:\n",
        "                print('\\t{}_output_sup{}_activation'.format(name, i-1))\n",
        "                \n",
        "                if output_activation == 'Sigmoid':\n",
        "                    X = Activation('sigmoid', name='{}_output_sup{}_activation'.format(name, i-1))(X)\n",
        "                else:\n",
        "                    activation_func = eval(output_activation)\n",
        "                    X = activation_func(name='{}_output_sup{}_activation'.format(name, i-1))(X)\n",
        "            else:\n",
        "                if unpool is False:\n",
        "                    print('\\t{}_output_sup{}_trans_conv'.format(name, i-1))\n",
        "                else:\n",
        "                    print('\\t{}_output_sup{}_unpool'.format(name, i-1))\n",
        "                    \n",
        "            OUT_stack.append(X)\n",
        "        \n",
        "        X = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n",
        "                        activation=output_activation, name='{}_output_final'.format(name))\n",
        "        OUT_stack.append(X)\n",
        "        \n",
        "        \n",
        "        concat_out = concatenate([OUT_stack[0],OUT_stack[1] , OUT_stack[2] , OUT_stack[4]], axis=-1)\n",
        "        \n",
        "        X = CONV_output(concat_out, n_labels, kernel_size=3, \n",
        "                        activation=output_activation, name='{}concate_output1'.format(name))\n",
        "        OUT_stack.append(X)        \n",
        "        \n",
        "        if output_activation:\n",
        "            print('\\t{}_output_final_activation'.format(name))\n",
        "        else:\n",
        "            print('\\t{}_output_final'.format(name))\n",
        "            \n",
        "        model = Model([IN,], OUT_stack)\n",
        "\n",
        "    else:\n",
        "        OUT = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n",
        "                          activation=output_activation, name='{}_output_final'.format(name))\n",
        "\n",
        "        model = Model([IN,], [OUT,])\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def freeze_model(model, freeze_batch_norm=False):\n",
        "    '''\n",
        "    freeze a keras model\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        model: a keras model\n",
        "        freeze_batch_norm: False for not freezing batch notmalization layers\n",
        "    '''\n",
        "    if freeze_batch_norm:\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        from tensorflow.keras.layers import BatchNormalization    \n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, BatchNormalization):\n",
        "                layer.trainable = True\n",
        "            else:\n",
        "                layer.trainable = False\n",
        "    return model\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    name = 'unet3plus'\n",
        "    activation = 'ReLU'\n",
        "    filter_num_down = [32, 64, 128, 256, 512 , 1024]\n",
        "    filter_num_skip = [64, 64, 64, 64 , 64]\n",
        "    filter_num_aggregate = 160\n",
        "\n",
        "    stack_num_down = 2\n",
        "    stack_num_up = 1\n",
        "    n_labels = 1\n",
        "    input_shape = (256, 256, 3)\n",
        "    \n",
        "    \n",
        "    model = unet_3plus_2d(input_shape , n_labels, filter_num_down, filter_num_skip=filter_num_skip, filter_num_aggregate=filter_num_aggregate, \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
        "                  batch_norm=True, pool=True, unpool=False, deep_supervision=False, \n",
        "                  backbone= \"DenseNet201\", weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n",
        "    model.summary()\n",
        "    model_checkpoint1 = keras.callbacks.ModelCheckpoint('unet3Plus.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n",
        "    csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\n",
        "    model.compile(optimizer=Adam(lr=lrate), loss=bce_jaccard_loss , metrics=[ACL5 ,bce_jaccard_loss , dice_coef , dsc,  dice_loss,iou_coeff,precision,recall])\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:39.559579Z",
          "iopub.execute_input": "2021-09-17T08:41:39.560155Z",
          "iopub.status.idle": "2021-09-17T08:41:50.745004Z",
          "shell.execute_reply.started": "2021-09-17T08:41:39.560120Z",
          "shell.execute_reply": "2021-09-17T08:41:50.744325Z"
        },
        "trusted": true,
        "id": "icJ6GCguWAyk",
        "outputId": "a472849b-7f87-4268-d598-4cdc0b55f259"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2021-09-17 08:41:39.642936: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-17 08:41:39.645721: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n2021-09-17 08:41:39.683492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.684099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-17 08:41:39.684171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 08:41:39.713727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 08:41:39.713811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-17 08:41:39.728215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-17 08:41:39.748188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-17 08:41:39.778062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-17 08:41:39.784992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-17 08:41:39.787476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 08:41:39.787674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.788387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.789565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-17 08:41:39.790037: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-09-17 08:41:39.790248: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2021-09-17 08:41:39.790418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.791010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2021-09-17 08:41:39.791050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 08:41:39.791078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 08:41:39.791096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2021-09-17 08:41:39.791113: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2021-09-17 08:41:39.791131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2021-09-17 08:41:39.791148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2021-09-17 08:41:39.791166: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2021-09-17 08:41:39.791197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 08:41:39.791276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.791898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:39.792414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2021-09-17 08:41:39.793165: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2021-09-17 08:41:41.190607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-09-17 08:41:41.190653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n2021-09-17 08:41:41.190667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n2021-09-17 08:41:41.194315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:41.195484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:41.199362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-09-17 08:41:41.200260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n74842112/74836368 [==============================] - 1s 0us/step\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n__________________________________________________________________________________________________\nDenseNet201_backbone (Functiona [(None, 128, 128, 64 18321984    input_1[0][0]                    \n__________________________________________________________________________________________________\nunet3plus_up_0_en0_trans_conv ( (None, 16, 16, 64)   1105984     DenseNet201_backbone[0][4]       \n__________________________________________________________________________________________________\nunet3plus_up_0_en0_bn (BatchNor (None, 16, 16, 64)   256         unet3plus_up_0_en0_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_0_en0_activation ( (None, 16, 16, 64)   0           unet3plus_up_0_en0_bn[0][0]      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 16, 16, 64)   0           unet3plus_up_0_en0_activation[0][\n__________________________________________________________________________________________________\nunet3plus_down_0_en2_maxpool (M (None, 16, 16, 256)  0           DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\nunet3plus_down_0_en3_maxpool (M (None, 16, 16, 128)  0           DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_avepoo (None, 64)           0           dropout[0][0]                    \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_avepoo (None, 896)          0           DenseNet201_backbone[0][3]       \n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_avepoo (None, 256)          0           unet3plus_down_0_en2_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_avepoo (None, 128)          0           unet3plus_down_0_en3_maxpool[0][0\n__________________________________________________________________________________________________\ntf.expand_dims (TFOpLambda)     (None, 1, 64)        0           unet3plus_down_from0_to0_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   576         dropout[0][0]                    \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   576         dropout[0][0]                    \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   576         dropout[0][0]                    \n__________________________________________________________________________________________________\ntf.expand_dims_2 (TFOpLambda)   (None, 1, 896)       0           unet3plus_down_from0_to1_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  8064        DenseNet201_backbone[0][3]       \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  8064        DenseNet201_backbone[0][3]       \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  8064        DenseNet201_backbone[0][3]       \n__________________________________________________________________________________________________\ntf.expand_dims_4 (TFOpLambda)   (None, 1, 256)       0           unet3plus_down_from0_to2_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  2304        unet3plus_down_0_en2_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  2304        unet3plus_down_0_en2_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  2304        unet3plus_down_0_en2_maxpool[0][0\n__________________________________________________________________________________________________\ntf.expand_dims_6 (TFOpLambda)   (None, 1, 128)       0           unet3plus_down_from0_to3_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  1152        unet3plus_down_0_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  1152        unet3plus_down_0_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  1152        unet3plus_down_0_en3_maxpool[0][0\n__________________________________________________________________________________________________\ntf.expand_dims_1 (TFOpLambda)   (None, 1, 1, 64)     0           tf.expand_dims[0][0]             \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_3 (TFOpLambda)   (None, 1, 1, 896)    0           tf.expand_dims_2[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  3584        unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  3584        unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  3584        unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_5 (TFOpLambda)   (None, 1, 1, 256)    0           tf.expand_dims_4[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  1024        unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  1024        unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  1024        unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_7 (TFOpLambda)   (None, 1, 1, 128)    0           tf.expand_dims_6[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  512         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  512         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  512         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_1[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 1, 1, 64)     57344       tf.expand_dims_3[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 896)  0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 1, 1, 64)     16384       tf.expand_dims_5[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 256)  0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 1, 1, 64)     8192        tf.expand_dims_7[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 128)  0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 1, 1, 64)     256         unet3plus_down_from0_to0_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 16, 16, 64)   4096        dropout[0][0]                    \n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   4096        unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   4096        unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   4096        unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 1, 1, 64)     256         unet3plus_down_from0_to1_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 16, 16, 64)   57344       DenseNet201_backbone[0][3]       \n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   57344       unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   57344       unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   57344       unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 1, 1, 64)     256         unet3plus_down_from0_to2_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 16, 16, 64)   16384       unet3plus_down_0_en2_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   16384       unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   16384       unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   16384       unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 1, 1, 64)     256         unet3plus_down_from0_to3_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 16, 16, 64)   8192        unet3plus_down_0_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   8192        unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   8192        unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   8192        unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 1, 1, 64)     0           unet3plus_down_from0_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 16, 16, 64)   256         unet3plus_down_from0_to0_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 1, 1, 64)     0           unet3plus_down_from0_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 16, 16, 64)   256         unet3plus_down_from0_to1_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 1, 1, 64)     0           unet3plus_down_from0_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 16, 16, 64)   256         unet3plus_down_from0_to2_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 1, 1, 64)     0           unet3plus_down_from0_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 16, 16, 64)   256         unet3plus_down_from0_to3_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   256         unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_resize (None, 16, 16, 64)   0           unet3plus_down_from0_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_conv_b (None, 16, 16, 64)   0           unet3plus_down_from0_to0_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to0_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_resize (None, 16, 16, 64)   0           unet3plus_down_from0_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_conv_b (None, 16, 16, 64)   0           unet3plus_down_from0_to1_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to1_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_resize (None, 16, 16, 64)   0           unet3plus_down_from0_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_conv_b (None, 16, 16, 64)   0           unet3plus_down_from0_to2_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to2_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_resize (None, 16, 16, 64)   0           unet3plus_down_from0_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_conv_b (None, 16, 16, 64)   0           unet3plus_down_from0_to3_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from0_to3_sepcon (None, 16, 16, 64)   0           unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 16, 16, 320)  0           unet3plus_down_from0_to0_resize_b\n                                                                 unet3plus_down_from0_to0_conv_b0_\n                                                                 unet3plus_down_from0_to0_sepconv_\n                                                                 unet3plus_down_from0_to0_sepconv_\n                                                                 unet3plus_down_from0_to0_sepconv_\n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 16, 16, 320)  0           unet3plus_down_from0_to1_resize_b\n                                                                 unet3plus_down_from0_to1_conv_b0_\n                                                                 unet3plus_down_from0_to1_sepconv_\n                                                                 unet3plus_down_from0_to1_sepconv_\n                                                                 unet3plus_down_from0_to1_sepconv_\n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 16, 16, 320)  0           unet3plus_down_from0_to2_resize_b\n                                                                 unet3plus_down_from0_to2_conv_b0_\n                                                                 unet3plus_down_from0_to2_sepconv_\n                                                                 unet3plus_down_from0_to2_sepconv_\n                                                                 unet3plus_down_from0_to2_sepconv_\n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 16, 16, 320)  0           unet3plus_down_from0_to3_resize_b\n                                                                 unet3plus_down_from0_to3_conv_b0_\n                                                                 unet3plus_down_from0_to3_sepconv_\n                                                                 unet3plus_down_from0_to3_sepconv_\n                                                                 unet3plus_down_from0_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_concat_0 (Concatenate (None, 16, 16, 1280) 0           concatenate[0][0]                \n                                                                 concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n                                                                 concatenate_3[0][0]              \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_0_0 (Conv (None, 16, 16, 160)  1843200     unet3plus_concat_0[0][0]         \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_0_0_bn (B (None, 16, 16, 160)  640         unet3plus_fusion_conv_0_0[0][0]  \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_0_0_activ (None, 16, 16, 160)  0           unet3plus_fusion_conv_0_0_bn[0][0\n__________________________________________________________________________________________________\nunet3plus_up_1_en0_trans_conv ( (None, 32, 32, 64)   1105984     DenseNet201_backbone[0][4]       \n__________________________________________________________________________________________________\nunet3plus_up_1_en1_trans_conv ( (None, 32, 32, 64)   92224       unet3plus_fusion_conv_0_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_1_en0_bn (BatchNor (None, 32, 32, 64)   256         unet3plus_up_1_en0_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_1_en1_bn (BatchNor (None, 32, 32, 64)   256         unet3plus_up_1_en1_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_1_en0_activation ( (None, 32, 32, 64)   0           unet3plus_up_1_en0_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_1_en1_activation ( (None, 32, 32, 64)   0           unet3plus_up_1_en1_bn[0][0]      \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 32, 32, 64)   0           unet3plus_up_1_en0_activation[0][\n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 32, 32, 64)   0           unet3plus_up_1_en1_activation[0][\n__________________________________________________________________________________________________\nunet3plus_down_1_en3_maxpool (M (None, 32, 32, 128)  0           DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_avepoo (None, 64)           0           dropout_1[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_avepoo (None, 64)           0           dropout_2[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_avepoo (None, 256)          0           DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_avepoo (None, 128)          0           unet3plus_down_1_en3_maxpool[0][0\n__________________________________________________________________________________________________\ntf.expand_dims_8 (TFOpLambda)   (None, 1, 64)        0           unet3plus_down_from1_to0_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   576         dropout_1[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   576         dropout_1[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   576         dropout_1[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_10 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from1_to1_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   576         dropout_2[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   576         dropout_2[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   576         dropout_2[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_12 (TFOpLambda)  (None, 1, 256)       0           unet3plus_down_from1_to2_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  2304        DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  2304        DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  2304        DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\ntf.expand_dims_14 (TFOpLambda)  (None, 1, 128)       0           unet3plus_down_from1_to3_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  1152        unet3plus_down_1_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  1152        unet3plus_down_1_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  1152        unet3plus_down_1_en3_maxpool[0][0\n__________________________________________________________________________________________________\ntf.expand_dims_9 (TFOpLambda)   (None, 1, 1, 64)     0           tf.expand_dims_8[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_11 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_10[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_13 (TFOpLambda)  (None, 1, 1, 256)    0           tf.expand_dims_12[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  1024        unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  1024        unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  1024        unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_15 (TFOpLambda)  (None, 1, 1, 128)    0           tf.expand_dims_14[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  512         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  512         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  512         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_9[0][0]           \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_11[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 1, 1, 64)     16384       tf.expand_dims_13[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 256)  0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 1, 1, 64)     8192        tf.expand_dims_15[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 128)  0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 1, 1, 64)     256         unet3plus_down_from1_to0_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 32, 32, 64)   4096        dropout_1[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 1, 1, 64)     256         unet3plus_down_from1_to1_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 32, 32, 64)   4096        dropout_2[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   4096        unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 1, 1, 64)     256         unet3plus_down_from1_to2_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 32, 32, 64)   16384       DenseNet201_backbone[0][2]       \n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   16384       unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   16384       unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   16384       unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 1, 1, 64)     256         unet3plus_down_from1_to3_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 32, 32, 64)   8192        unet3plus_down_1_en3_maxpool[0][0\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   8192        unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   8192        unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   8192        unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 1, 1, 64)     0           unet3plus_down_from1_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 32, 32, 64)   256         unet3plus_down_from1_to0_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 1, 1, 64)     0           unet3plus_down_from1_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 32, 32, 64)   256         unet3plus_down_from1_to1_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 1, 1, 64)     0           unet3plus_down_from1_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 32, 32, 64)   256         unet3plus_down_from1_to2_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 1, 1, 64)     0           unet3plus_down_from1_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 32, 32, 64)   256         unet3plus_down_from1_to3_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   256         unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_resize (None, 32, 32, 64)   0           unet3plus_down_from1_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_conv_b (None, 32, 32, 64)   0           unet3plus_down_from1_to0_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to0_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_resize (None, 32, 32, 64)   0           unet3plus_down_from1_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_conv_b (None, 32, 32, 64)   0           unet3plus_down_from1_to1_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to1_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_resize (None, 32, 32, 64)   0           unet3plus_down_from1_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_conv_b (None, 32, 32, 64)   0           unet3plus_down_from1_to2_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to2_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_resize (None, 32, 32, 64)   0           unet3plus_down_from1_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_conv_b (None, 32, 32, 64)   0           unet3plus_down_from1_to3_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from1_to3_sepcon (None, 32, 32, 64)   0           unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 32, 32, 320)  0           unet3plus_down_from1_to0_resize_b\n                                                                 unet3plus_down_from1_to0_conv_b0_\n                                                                 unet3plus_down_from1_to0_sepconv_\n                                                                 unet3plus_down_from1_to0_sepconv_\n                                                                 unet3plus_down_from1_to0_sepconv_\n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 32, 32, 320)  0           unet3plus_down_from1_to1_resize_b\n                                                                 unet3plus_down_from1_to1_conv_b0_\n                                                                 unet3plus_down_from1_to1_sepconv_\n                                                                 unet3plus_down_from1_to1_sepconv_\n                                                                 unet3plus_down_from1_to1_sepconv_\n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 32, 32, 320)  0           unet3plus_down_from1_to2_resize_b\n                                                                 unet3plus_down_from1_to2_conv_b0_\n                                                                 unet3plus_down_from1_to2_sepconv_\n                                                                 unet3plus_down_from1_to2_sepconv_\n                                                                 unet3plus_down_from1_to2_sepconv_\n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 32, 32, 320)  0           unet3plus_down_from1_to3_resize_b\n                                                                 unet3plus_down_from1_to3_conv_b0_\n                                                                 unet3plus_down_from1_to3_sepconv_\n                                                                 unet3plus_down_from1_to3_sepconv_\n                                                                 unet3plus_down_from1_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_concat_1 (Concatenate (None, 32, 32, 1280) 0           concatenate_4[0][0]              \n                                                                 concatenate_5[0][0]              \n                                                                 concatenate_6[0][0]              \n                                                                 concatenate_7[0][0]              \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_1_0 (Conv (None, 32, 32, 160)  1843200     unet3plus_concat_1[0][0]         \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_1_0_bn (B (None, 32, 32, 160)  640         unet3plus_fusion_conv_1_0[0][0]  \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_1_0_activ (None, 32, 32, 160)  0           unet3plus_fusion_conv_1_0_bn[0][0\n__________________________________________________________________________________________________\nunet3plus_up_2_en0_trans_conv ( (None, 64, 64, 64)   1105984     DenseNet201_backbone[0][4]       \n__________________________________________________________________________________________________\nunet3plus_up_2_en1_trans_conv ( (None, 64, 64, 64)   92224       unet3plus_fusion_conv_0_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_2_en2_trans_conv ( (None, 64, 64, 64)   92224       unet3plus_fusion_conv_1_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_2_en0_bn (BatchNor (None, 64, 64, 64)   256         unet3plus_up_2_en0_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_2_en1_bn (BatchNor (None, 64, 64, 64)   256         unet3plus_up_2_en1_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_2_en2_bn (BatchNor (None, 64, 64, 64)   256         unet3plus_up_2_en2_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_2_en0_activation ( (None, 64, 64, 64)   0           unet3plus_up_2_en0_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_2_en1_activation ( (None, 64, 64, 64)   0           unet3plus_up_2_en1_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_2_en2_activation ( (None, 64, 64, 64)   0           unet3plus_up_2_en2_bn[0][0]      \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 64, 64, 64)   0           unet3plus_up_2_en0_activation[0][\n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 64, 64, 64)   0           unet3plus_up_2_en1_activation[0][\n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 64, 64, 64)   0           unet3plus_up_2_en2_activation[0][\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_avepoo (None, 64)           0           dropout_3[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_avepoo (None, 64)           0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_avepoo (None, 64)           0           dropout_5[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_avepoo (None, 128)          0           DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\ntf.expand_dims_16 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from2_to0_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   576         dropout_3[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   576         dropout_3[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   576         dropout_3[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_18 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from2_to1_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   576         dropout_4[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   576         dropout_4[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   576         dropout_4[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_20 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from2_to2_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   576         dropout_5[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   576         dropout_5[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   576         dropout_5[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_22 (TFOpLambda)  (None, 1, 128)       0           unet3plus_down_from2_to3_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  1152        DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  1152        DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  1152        DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\ntf.expand_dims_17 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_16[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_19 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_18[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_21 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_20[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_23 (TFOpLambda)  (None, 1, 1, 128)    0           tf.expand_dims_22[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  512         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  512         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  512         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_17[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_19[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_21[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 1, 1, 64)     8192        tf.expand_dims_23[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 128)  0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 1, 1, 64)     256         unet3plus_down_from2_to0_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 64, 64, 64)   4096        dropout_3[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 1, 1, 64)     256         unet3plus_down_from2_to1_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 64, 64, 64)   4096        dropout_4[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 1, 1, 64)     256         unet3plus_down_from2_to2_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 64, 64, 64)   4096        dropout_5[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   4096        unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 1, 1, 64)     256         unet3plus_down_from2_to3_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 64, 64, 64)   8192        DenseNet201_backbone[0][1]       \n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   8192        unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   8192        unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   8192        unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 1, 1, 64)     0           unet3plus_down_from2_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 64, 64, 64)   256         unet3plus_down_from2_to0_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 1, 1, 64)     0           unet3plus_down_from2_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 64, 64, 64)   256         unet3plus_down_from2_to1_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 1, 1, 64)     0           unet3plus_down_from2_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 64, 64, 64)   256         unet3plus_down_from2_to2_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 1, 1, 64)     0           unet3plus_down_from2_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 64, 64, 64)   256         unet3plus_down_from2_to3_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   256         unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_resize (None, 64, 64, 64)   0           unet3plus_down_from2_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_conv_b (None, 64, 64, 64)   0           unet3plus_down_from2_to0_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to0_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_resize (None, 64, 64, 64)   0           unet3plus_down_from2_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_conv_b (None, 64, 64, 64)   0           unet3plus_down_from2_to1_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to1_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_resize (None, 64, 64, 64)   0           unet3plus_down_from2_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_conv_b (None, 64, 64, 64)   0           unet3plus_down_from2_to2_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to2_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_resize (None, 64, 64, 64)   0           unet3plus_down_from2_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_conv_b (None, 64, 64, 64)   0           unet3plus_down_from2_to3_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from2_to3_sepcon (None, 64, 64, 64)   0           unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nconcatenate_8 (Concatenate)     (None, 64, 64, 320)  0           unet3plus_down_from2_to0_resize_b\n                                                                 unet3plus_down_from2_to0_conv_b0_\n                                                                 unet3plus_down_from2_to0_sepconv_\n                                                                 unet3plus_down_from2_to0_sepconv_\n                                                                 unet3plus_down_from2_to0_sepconv_\n__________________________________________________________________________________________________\nconcatenate_9 (Concatenate)     (None, 64, 64, 320)  0           unet3plus_down_from2_to1_resize_b\n                                                                 unet3plus_down_from2_to1_conv_b0_\n                                                                 unet3plus_down_from2_to1_sepconv_\n                                                                 unet3plus_down_from2_to1_sepconv_\n                                                                 unet3plus_down_from2_to1_sepconv_\n__________________________________________________________________________________________________\nconcatenate_10 (Concatenate)    (None, 64, 64, 320)  0           unet3plus_down_from2_to2_resize_b\n                                                                 unet3plus_down_from2_to2_conv_b0_\n                                                                 unet3plus_down_from2_to2_sepconv_\n                                                                 unet3plus_down_from2_to2_sepconv_\n                                                                 unet3plus_down_from2_to2_sepconv_\n__________________________________________________________________________________________________\nconcatenate_11 (Concatenate)    (None, 64, 64, 320)  0           unet3plus_down_from2_to3_resize_b\n                                                                 unet3plus_down_from2_to3_conv_b0_\n                                                                 unet3plus_down_from2_to3_sepconv_\n                                                                 unet3plus_down_from2_to3_sepconv_\n                                                                 unet3plus_down_from2_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_concat_2 (Concatenate (None, 64, 64, 1280) 0           concatenate_8[0][0]              \n                                                                 concatenate_9[0][0]              \n                                                                 concatenate_10[0][0]             \n                                                                 concatenate_11[0][0]             \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_2_0 (Conv (None, 64, 64, 160)  1843200     unet3plus_concat_2[0][0]         \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_2_0_bn (B (None, 64, 64, 160)  640         unet3plus_fusion_conv_2_0[0][0]  \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_2_0_activ (None, 64, 64, 160)  0           unet3plus_fusion_conv_2_0_bn[0][0\n__________________________________________________________________________________________________\nunet3plus_up_3_en0_trans_conv ( (None, 128, 128, 64) 1105984     DenseNet201_backbone[0][4]       \n__________________________________________________________________________________________________\nunet3plus_up_3_en1_trans_conv ( (None, 128, 128, 64) 92224       unet3plus_fusion_conv_0_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_3_en2_trans_conv ( (None, 128, 128, 64) 92224       unet3plus_fusion_conv_1_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_3_en3_trans_conv ( (None, 128, 128, 64) 92224       unet3plus_fusion_conv_2_0_activat\n__________________________________________________________________________________________________\nunet3plus_up_3_en0_bn (BatchNor (None, 128, 128, 64) 256         unet3plus_up_3_en0_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_3_en1_bn (BatchNor (None, 128, 128, 64) 256         unet3plus_up_3_en1_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_3_en2_bn (BatchNor (None, 128, 128, 64) 256         unet3plus_up_3_en2_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_3_en3_bn (BatchNor (None, 128, 128, 64) 256         unet3plus_up_3_en3_trans_conv[0][\n__________________________________________________________________________________________________\nunet3plus_up_3_en0_activation ( (None, 128, 128, 64) 0           unet3plus_up_3_en0_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_3_en1_activation ( (None, 128, 128, 64) 0           unet3plus_up_3_en1_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_3_en2_activation ( (None, 128, 128, 64) 0           unet3plus_up_3_en2_bn[0][0]      \n__________________________________________________________________________________________________\nunet3plus_up_3_en3_activation ( (None, 128, 128, 64) 0           unet3plus_up_3_en3_bn[0][0]      \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 128, 128, 64) 0           unet3plus_up_3_en0_activation[0][\n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 128, 128, 64) 0           unet3plus_up_3_en1_activation[0][\n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 128, 128, 64) 0           unet3plus_up_3_en2_activation[0][\n__________________________________________________________________________________________________\ndropout_9 (Dropout)             (None, 128, 128, 64) 0           unet3plus_up_3_en3_activation[0][\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_avepoo (None, 64)           0           dropout_6[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_avepoo (None, 64)           0           dropout_7[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_avepoo (None, 64)           0           dropout_8[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_avepoo (None, 64)           0           dropout_9[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_24 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from3_to0_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 576         dropout_6[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 576         dropout_6[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 576         dropout_6[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_26 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from3_to1_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 576         dropout_7[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 576         dropout_7[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 576         dropout_7[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_28 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from3_to2_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 576         dropout_8[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 576         dropout_8[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 576         dropout_8[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_30 (TFOpLambda)  (None, 1, 64)        0           unet3plus_down_from3_to3_avepool_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 576         dropout_9[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 576         dropout_9[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 576         dropout_9[0][0]                  \n__________________________________________________________________________________________________\ntf.expand_dims_25 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_24[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_27 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_26[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_29 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_28[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\ntf.expand_dims_31 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_30[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_25[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_27[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_29[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 1, 1, 64)     4096        tf.expand_dims_31[0][0]          \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 1, 1, 64)     256         unet3plus_down_from3_to0_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 128, 128, 64) 4096        dropout_6[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 1, 1, 64)     256         unet3plus_down_from3_to1_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 128, 128, 64) 4096        dropout_7[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 1, 1, 64)     256         unet3plus_down_from3_to2_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 128, 128, 64) 4096        dropout_8[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 1, 1, 64)     256         unet3plus_down_from3_to3_conv_b4[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 128, 128, 64) 4096        dropout_9[0][0]                  \n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 4096        unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 1, 1, 64)     0           unet3plus_down_from3_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 128, 128, 64) 256         unet3plus_down_from3_to0_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 1, 1, 64)     0           unet3plus_down_from3_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 128, 128, 64) 256         unet3plus_down_from3_to1_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 1, 1, 64)     0           unet3plus_down_from3_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 128, 128, 64) 256         unet3plus_down_from3_to2_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 1, 1, 64)     0           unet3plus_down_from3_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 128, 128, 64) 256         unet3plus_down_from3_to3_conv_b0[\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 256         unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_resize (None, 128, 128, 64) 0           unet3plus_down_from3_to0_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_conv_b (None, 128, 128, 64) 0           unet3plus_down_from3_to0_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to0_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_resize (None, 128, 128, 64) 0           unet3plus_down_from3_to1_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_conv_b (None, 128, 128, 64) 0           unet3plus_down_from3_to1_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to1_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_resize (None, 128, 128, 64) 0           unet3plus_down_from3_to2_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_conv_b (None, 128, 128, 64) 0           unet3plus_down_from3_to2_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to2_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_resize (None, 128, 128, 64) 0           unet3plus_down_from3_to3_conv_b4_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_conv_b (None, 128, 128, 64) 0           unet3plus_down_from3_to3_conv_b0_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_down_from3_to3_sepcon (None, 128, 128, 64) 0           unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 128, 128, 320 0           unet3plus_down_from3_to0_resize_b\n                                                                 unet3plus_down_from3_to0_conv_b0_\n                                                                 unet3plus_down_from3_to0_sepconv_\n                                                                 unet3plus_down_from3_to0_sepconv_\n                                                                 unet3plus_down_from3_to0_sepconv_\n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 128, 128, 320 0           unet3plus_down_from3_to1_resize_b\n                                                                 unet3plus_down_from3_to1_conv_b0_\n                                                                 unet3plus_down_from3_to1_sepconv_\n                                                                 unet3plus_down_from3_to1_sepconv_\n                                                                 unet3plus_down_from3_to1_sepconv_\n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 128, 128, 320 0           unet3plus_down_from3_to2_resize_b\n                                                                 unet3plus_down_from3_to2_conv_b0_\n                                                                 unet3plus_down_from3_to2_sepconv_\n                                                                 unet3plus_down_from3_to2_sepconv_\n                                                                 unet3plus_down_from3_to2_sepconv_\n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 128, 128, 320 0           unet3plus_down_from3_to3_resize_b\n                                                                 unet3plus_down_from3_to3_conv_b0_\n                                                                 unet3plus_down_from3_to3_sepconv_\n                                                                 unet3plus_down_from3_to3_sepconv_\n                                                                 unet3plus_down_from3_to3_sepconv_\n__________________________________________________________________________________________________\nunet3plus_concat_3 (Concatenate (None, 128, 128, 128 0           concatenate_12[0][0]             \n                                                                 concatenate_13[0][0]             \n                                                                 concatenate_14[0][0]             \n                                                                 concatenate_15[0][0]             \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_3_0 (Conv (None, 128, 128, 160 1843200     unet3plus_concat_3[0][0]         \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_3_0_bn (B (None, 128, 128, 160 640         unet3plus_fusion_conv_3_0[0][0]  \n__________________________________________________________________________________________________\nunet3plus_fusion_conv_3_0_activ (None, 128, 128, 160 0           unet3plus_fusion_conv_3_0_bn[0][0\n__________________________________________________________________________________________________\nunet3plus_plain_up4_decode_tran (None, 256, 256, 160 230560      unet3plus_fusion_conv_3_0_activat\n__________________________________________________________________________________________________\nunet3plus_plain_up4_decode_bn ( (None, 256, 256, 160 640         unet3plus_plain_up4_decode_trans_\n__________________________________________________________________________________________________\nunet3plus_plain_up4_decode_acti (None, 256, 256, 160 0           unet3plus_plain_up4_decode_bn[0][\n__________________________________________________________________________________________________\ndropout_10 (Dropout)            (None, 256, 256, 160 0           unet3plus_plain_up4_decode_activa\n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_before (None, 256, 256, 160 230400      dropout_10[0][0]                 \n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_before (None, 256, 256, 160 640         unet3plus_plain_up4_conv_before_c\n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_before (None, 256, 256, 160 0           unet3plus_plain_up4_conv_before_c\n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_after_ (None, 256, 256, 160 230400      unet3plus_plain_up4_conv_before_c\n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_after_ (None, 256, 256, 160 640         unet3plus_plain_up4_conv_after_co\n__________________________________________________________________________________________________\nunet3plus_plain_up4_conv_after_ (None, 256, 256, 160 0           unet3plus_plain_up4_conv_after_co\n__________________________________________________________________________________________________\nunet3plus_output_final (Conv2D) (None, 256, 256, 1)  1441        unet3plus_plain_up4_conv_after_co\n__________________________________________________________________________________________________\nunet3plus_output_final_activati (None, 256, 256, 1)  0           unet3plus_output_final[0][0]     \n==================================================================================================\nTotal params: 32,265,473\nTrainable params: 13,915,137\nNon-trainable params: 18,350,336\n__________________________________________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BRDjuCC6WAym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_data,\n",
        "                              steps_per_epoch=1912,epochs=40,\n",
        "                              validation_steps=207,\n",
        "                              validation_data=valid_data,\n",
        "                              callbacks=[model_checkpoint1,csv_logger])"
      ],
      "metadata": {
        "id": "DJLOnaYejXrm",
        "outputId": "deabad6b-2cf2-449a-8124-ea3bd1e2a072",
        "execution": {
          "iopub.status.busy": "2021-09-17T08:41:50.746856Z",
          "iopub.execute_input": "2021-09-17T08:41:50.747114Z",
          "iopub.status.idle": "2021-09-17T15:39:49.324387Z",
          "shell.execute_reply.started": "2021-09-17T08:41:50.747079Z",
          "shell.execute_reply": "2021-09-17T15:39:49.323356Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 600 images belonging to 1 classes.\nFound 600 images belonging to 1 classes.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2021-09-17 08:41:51.513627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2021-09-17 08:41:51.517647: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000185000 Hz\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/40\n(None, 256, 256, 1)\n(None, 256, 256, 1)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2021-09-17 08:42:18.038952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2021-09-17 08:42:23.189604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2021-09-17 08:42:23.862725: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "1912/1912 [==============================] - ETA: 0s - loss: 0.6403 - ACL5: 26872.5016 - binary_crossentropy_plus_jaccard_loss: 0.6403 - dice_coef: 0.5398 - dsc: 0.5622 - dice_loss: 0.4378 - iou_coeff: 0.4278 - precision: 0.6076 - recall: 0.6500Found 200 images belonging to 1 classes.\nFound 200 images belonging to 1 classes.\n(None, 256, 256, 1)\n1912/1912 [==============================] - 667s 331ms/step - loss: 0.6402 - ACL5: 26863.4731 - binary_crossentropy_plus_jaccard_loss: 0.6402 - dice_coef: 0.5399 - dsc: 0.5623 - dice_loss: 0.4377 - iou_coeff: 0.4278 - precision: 0.6076 - recall: 0.6501 - val_loss: 0.6629 - val_ACL5: 2878.2725 - val_binary_crossentropy_plus_jaccard_loss: 0.6629 - val_dice_coef: 0.5189 - val_dsc: 0.5189 - val_dice_loss: 0.4811 - val_iou_coeff: 0.4149 - val_precision: 0.6653 - val_recall: 0.4577\n\nEpoch 00001: val_dice_loss improved from inf to 0.48110, saving model to unet3Plus.hdf5\nEpoch 2/40\n1912/1912 [==============================] - 629s 329ms/step - loss: 0.4069 - ACL5: 4439.8259 - binary_crossentropy_plus_jaccard_loss: 0.4069 - dice_coef: 0.7293 - dsc: 0.7609 - dice_loss: 0.2391 - iou_coeff: 0.6231 - precision: 0.7808 - recall: 0.7731 - val_loss: 0.5260 - val_ACL5: 2381.1011 - val_binary_crossentropy_plus_jaccard_loss: 0.5260 - val_dice_coef: 0.6418 - val_dsc: 0.6418 - val_dice_loss: 0.3582 - val_iou_coeff: 0.5335 - val_precision: 0.7264 - val_recall: 0.5997\n\nEpoch 00002: val_dice_loss improved from 0.48110 to 0.35820, saving model to unet3Plus.hdf5\nEpoch 3/40\n1912/1912 [==============================] - 627s 328ms/step - loss: 0.3718 - ACL5: 4000.7319 - binary_crossentropy_plus_jaccard_loss: 0.3718 - dice_coef: 0.7543 - dsc: 0.7833 - dice_loss: 0.2167 - iou_coeff: 0.6552 - precision: 0.7969 - recall: 0.7932 - val_loss: 0.5598 - val_ACL5: 2408.2241 - val_binary_crossentropy_plus_jaccard_loss: 0.5598 - val_dice_coef: 0.6145 - val_dsc: 0.6145 - val_dice_loss: 0.3855 - val_iou_coeff: 0.5152 - val_precision: 0.6881 - val_recall: 0.5536\n\nEpoch 00003: val_dice_loss did not improve from 0.35820\nEpoch 4/40\n1912/1912 [==============================] - 622s 325ms/step - loss: 0.3443 - ACL5: 3754.1487 - binary_crossentropy_plus_jaccard_loss: 0.3443 - dice_coef: 0.7796 - dsc: 0.8044 - dice_loss: 0.1956 - iou_coeff: 0.6807 - precision: 0.8151 - recall: 0.8144 - val_loss: 0.5074 - val_ACL5: 2297.4851 - val_binary_crossentropy_plus_jaccard_loss: 0.5074 - val_dice_coef: 0.6777 - val_dsc: 0.6777 - val_dice_loss: 0.3223 - val_iou_coeff: 0.5645 - val_precision: 0.7844 - val_recall: 0.5879\n\nEpoch 00004: val_dice_loss improved from 0.35820 to 0.32231, saving model to unet3Plus.hdf5\nEpoch 5/40\n1912/1912 [==============================] - 621s 325ms/step - loss: 0.3302 - ACL5: 3564.2534 - binary_crossentropy_plus_jaccard_loss: 0.3302 - dice_coef: 0.7900 - dsc: 0.8132 - dice_loss: 0.1868 - iou_coeff: 0.6937 - precision: 0.8222 - recall: 0.8210 - val_loss: 0.5282 - val_ACL5: 2365.8684 - val_binary_crossentropy_plus_jaccard_loss: 0.5282 - val_dice_coef: 0.6444 - val_dsc: 0.6444 - val_dice_loss: 0.3556 - val_iou_coeff: 0.5433 - val_precision: 0.7126 - val_recall: 0.5994\n\nEpoch 00005: val_dice_loss did not improve from 0.32231\nEpoch 6/40\n1912/1912 [==============================] - 625s 327ms/step - loss: 0.3101 - ACL5: 3383.2304 - binary_crossentropy_plus_jaccard_loss: 0.3101 - dice_coef: 0.8060 - dsc: 0.8273 - dice_loss: 0.1727 - iou_coeff: 0.7124 - precision: 0.8354 - recall: 0.8348 - val_loss: 0.5733 - val_ACL5: 2463.9270 - val_binary_crossentropy_plus_jaccard_loss: 0.5733 - val_dice_coef: 0.6107 - val_dsc: 0.6107 - val_dice_loss: 0.3893 - val_iou_coeff: 0.5172 - val_precision: 0.6977 - val_recall: 0.5330\n\nEpoch 00006: val_dice_loss did not improve from 0.32231\nEpoch 7/40\n1912/1912 [==============================] - 623s 326ms/step - loss: 0.2990 - ACL5: 3250.4354 - binary_crossentropy_plus_jaccard_loss: 0.2990 - dice_coef: 0.8160 - dsc: 0.8348 - dice_loss: 0.1652 - iou_coeff: 0.7223 - precision: 0.8417 - recall: 0.8419 - val_loss: 0.5574 - val_ACL5: 2320.3838 - val_binary_crossentropy_plus_jaccard_loss: 0.5574 - val_dice_coef: 0.6354 - val_dsc: 0.6354 - val_dice_loss: 0.3646 - val_iou_coeff: 0.5423 - val_precision: 0.6868 - val_recall: 0.5612\n\nEpoch 00007: val_dice_loss did not improve from 0.32231\nEpoch 8/40\n1912/1912 [==============================] - 622s 326ms/step - loss: 0.2919 - ACL5: 3156.9306 - binary_crossentropy_plus_jaccard_loss: 0.2919 - dice_coef: 0.8218 - dsc: 0.8395 - dice_loss: 0.1605 - iou_coeff: 0.7297 - precision: 0.8440 - recall: 0.8467 - val_loss: 0.5051 - val_ACL5: 2215.2910 - val_binary_crossentropy_plus_jaccard_loss: 0.5051 - val_dice_coef: 0.6643 - val_dsc: 0.6643 - val_dice_loss: 0.3357 - val_iou_coeff: 0.5713 - val_precision: 0.7248 - val_recall: 0.6087\n\nEpoch 00008: val_dice_loss did not improve from 0.32231\nEpoch 9/40\n1912/1912 [==============================] - 622s 325ms/step - loss: 0.2821 - ACL5: 3075.2155 - binary_crossentropy_plus_jaccard_loss: 0.2821 - dice_coef: 0.8299 - dsc: 0.8458 - dice_loss: 0.1542 - iou_coeff: 0.7381 - precision: 0.8516 - recall: 0.8515 - val_loss: 0.4780 - val_ACL5: 2010.4731 - val_binary_crossentropy_plus_jaccard_loss: 0.4780 - val_dice_coef: 0.6885 - val_dsc: 0.6885 - val_dice_loss: 0.3115 - val_iou_coeff: 0.5933 - val_precision: 0.7326 - val_recall: 0.6386\n\nEpoch 00009: val_dice_loss improved from 0.32231 to 0.31149, saving model to unet3Plus.hdf5\nEpoch 10/40\n1912/1912 [==============================] - 621s 325ms/step - loss: 0.2714 - ACL5: 2946.6613 - binary_crossentropy_plus_jaccard_loss: 0.2714 - dice_coef: 0.8389 - dsc: 0.8529 - dice_loss: 0.1471 - iou_coeff: 0.7483 - precision: 0.8580 - recall: 0.8568 - val_loss: 0.5310 - val_ACL5: 2314.4045 - val_binary_crossentropy_plus_jaccard_loss: 0.5310 - val_dice_coef: 0.6608 - val_dsc: 0.6608 - val_dice_loss: 0.3392 - val_iou_coeff: 0.5622 - val_precision: 0.7167 - val_recall: 0.5833\n\nEpoch 00010: val_dice_loss did not improve from 0.31149\nEpoch 11/40\n1912/1912 [==============================] - 619s 324ms/step - loss: 0.2708 - ACL5: 2923.3895 - binary_crossentropy_plus_jaccard_loss: 0.2708 - dice_coef: 0.8447 - dsc: 0.8533 - dice_loss: 0.1467 - iou_coeff: 0.7502 - precision: 0.8568 - recall: 0.8559 - val_loss: 0.5393 - val_ACL5: 2329.8242 - val_binary_crossentropy_plus_jaccard_loss: 0.5393 - val_dice_coef: 0.6373 - val_dsc: 0.6373 - val_dice_loss: 0.3627 - val_iou_coeff: 0.5481 - val_precision: 0.6954 - val_recall: 0.5869\n\nEpoch 00011: val_dice_loss did not improve from 0.31149\nEpoch 12/40\n1912/1912 [==============================] - 621s 325ms/step - loss: 0.2659 - ACL5: 2877.4044 - binary_crossentropy_plus_jaccard_loss: 0.2659 - dice_coef: 0.8442 - dsc: 0.8552 - dice_loss: 0.1448 - iou_coeff: 0.7538 - precision: 0.8604 - recall: 0.8576 - val_loss: 0.5035 - val_ACL5: 2077.4224 - val_binary_crossentropy_plus_jaccard_loss: 0.5035 - val_dice_coef: 0.6809 - val_dsc: 0.6809 - val_dice_loss: 0.3191 - val_iou_coeff: 0.5866 - val_precision: 0.7085 - val_recall: 0.6106\n\nEpoch 00012: val_dice_loss did not improve from 0.31149\nEpoch 13/40\n1912/1912 [==============================] - 619s 324ms/step - loss: 0.2535 - ACL5: 2789.8462 - binary_crossentropy_plus_jaccard_loss: 0.2535 - dice_coef: 0.8547 - dsc: 0.8639 - dice_loss: 0.1361 - iou_coeff: 0.7647 - precision: 0.8687 - recall: 0.8677 - val_loss: 0.5442 - val_ACL5: 2418.0078 - val_binary_crossentropy_plus_jaccard_loss: 0.5442 - val_dice_coef: 0.6305 - val_dsc: 0.6305 - val_dice_loss: 0.3695 - val_iou_coeff: 0.5297 - val_precision: 0.7632 - val_recall: 0.5576\n\nEpoch 00013: val_dice_loss did not improve from 0.31149\nEpoch 14/40\n1912/1912 [==============================] - 619s 324ms/step - loss: 0.2527 - ACL5: 2740.9914 - binary_crossentropy_plus_jaccard_loss: 0.2527 - dice_coef: 0.8592 - dsc: 0.8651 - dice_loss: 0.1349 - iou_coeff: 0.7673 - precision: 0.8690 - recall: 0.8651 - val_loss: 0.4684 - val_ACL5: 1968.1729 - val_binary_crossentropy_plus_jaccard_loss: 0.4684 - val_dice_coef: 0.7107 - val_dsc: 0.7107 - val_dice_loss: 0.2893 - val_iou_coeff: 0.6111 - val_precision: 0.7633 - val_recall: 0.6265\n\nEpoch 00014: val_dice_loss improved from 0.31149 to 0.28931, saving model to unet3Plus.hdf5\nEpoch 15/40\n1912/1912 [==============================] - 619s 323ms/step - loss: 0.2448 - ACL5: 2678.7138 - binary_crossentropy_plus_jaccard_loss: 0.2448 - dice_coef: 0.8589 - dsc: 0.8696 - dice_loss: 0.1304 - iou_coeff: 0.7737 - precision: 0.8750 - recall: 0.8701 - val_loss: 0.6213 - val_ACL5: 2529.9189 - val_binary_crossentropy_plus_jaccard_loss: 0.6213 - val_dice_coef: 0.5856 - val_dsc: 0.5856 - val_dice_loss: 0.4144 - val_iou_coeff: 0.5039 - val_precision: 0.6501 - val_recall: 0.4981\n\nEpoch 00015: val_dice_loss did not improve from 0.28931\nEpoch 16/40\n1912/1912 [==============================] - 617s 323ms/step - loss: 0.2411 - ACL5: 2646.8849 - binary_crossentropy_plus_jaccard_loss: 0.2411 - dice_coef: 0.8627 - dsc: 0.8708 - dice_loss: 0.1292 - iou_coeff: 0.7760 - precision: 0.8777 - recall: 0.8724 - val_loss: 0.4474 - val_ACL5: 1991.5371 - val_binary_crossentropy_plus_jaccard_loss: 0.4474 - val_dice_coef: 0.7342 - val_dsc: 0.7342 - val_dice_loss: 0.2658 - val_iou_coeff: 0.6313 - val_precision: 0.7579 - val_recall: 0.6854\n\nEpoch 00016: val_dice_loss improved from 0.28931 to 0.26583, saving model to unet3Plus.hdf5\nEpoch 17/40\n1912/1912 [==============================] - 617s 323ms/step - loss: 0.2394 - ACL5: 2623.3305 - binary_crossentropy_plus_jaccard_loss: 0.2394 - dice_coef: 0.8673 - dsc: 0.8724 - dice_loss: 0.1276 - iou_coeff: 0.7787 - precision: 0.8759 - recall: 0.8740 - val_loss: 0.5358 - val_ACL5: 2306.5620 - val_binary_crossentropy_plus_jaccard_loss: 0.5358 - val_dice_coef: 0.6477 - val_dsc: 0.6477 - val_dice_loss: 0.3523 - val_iou_coeff: 0.5551 - val_precision: 0.6991 - val_recall: 0.5734\n\nEpoch 00017: val_dice_loss did not improve from 0.26583\nEpoch 18/40\n1912/1912 [==============================] - 618s 323ms/step - loss: 0.2298 - ACL5: 2541.7372 - binary_crossentropy_plus_jaccard_loss: 0.2298 - dice_coef: 0.8720 - dsc: 0.8784 - dice_loss: 0.1216 - iou_coeff: 0.7867 - precision: 0.8834 - recall: 0.8801 - val_loss: 0.5240 - val_ACL5: 2104.5339 - val_binary_crossentropy_plus_jaccard_loss: 0.5240 - val_dice_coef: 0.6420 - val_dsc: 0.6420 - val_dice_loss: 0.3580 - val_iou_coeff: 0.5489 - val_precision: 0.7042 - val_recall: 0.5754\n\nEpoch 00018: val_dice_loss did not improve from 0.26583\nEpoch 19/40\n1912/1912 [==============================] - 618s 323ms/step - loss: 0.2302 - ACL5: 2508.2452 - binary_crossentropy_plus_jaccard_loss: 0.2302 - dice_coef: 0.8727 - dsc: 0.8780 - dice_loss: 0.1220 - iou_coeff: 0.7875 - precision: 0.8818 - recall: 0.8780 - val_loss: 0.5352 - val_ACL5: 2388.2664 - val_binary_crossentropy_plus_jaccard_loss: 0.5352 - val_dice_coef: 0.6554 - val_dsc: 0.6554 - val_dice_loss: 0.3446 - val_iou_coeff: 0.5591 - val_precision: 0.7262 - val_recall: 0.5825\n\nEpoch 00019: val_dice_loss did not improve from 0.26583\nEpoch 20/40\n1912/1912 [==============================] - 619s 324ms/step - loss: 0.2243 - ACL5: 2463.6878 - binary_crossentropy_plus_jaccard_loss: 0.2243 - dice_coef: 0.8822 - dsc: 0.8834 - dice_loss: 0.1166 - iou_coeff: 0.7946 - precision: 0.8843 - recall: 0.8820 - val_loss: 0.5906 - val_ACL5: 2374.1775 - val_binary_crossentropy_plus_jaccard_loss: 0.5906 - val_dice_coef: 0.6016 - val_dsc: 0.6016 - val_dice_loss: 0.3984 - val_iou_coeff: 0.5118 - val_precision: 0.6983 - val_recall: 0.5096\n\nEpoch 00020: val_dice_loss did not improve from 0.26583\nEpoch 21/40\n1912/1912 [==============================] - 619s 324ms/step - loss: 0.2201 - ACL5: 2421.4734 - binary_crossentropy_plus_jaccard_loss: 0.2201 - dice_coef: 0.8827 - dsc: 0.8842 - dice_loss: 0.1158 - iou_coeff: 0.7972 - precision: 0.8877 - recall: 0.8825 - val_loss: 0.4659 - val_ACL5: 2057.2451 - val_binary_crossentropy_plus_jaccard_loss: 0.4659 - val_dice_coef: 0.7161 - val_dsc: 0.7161 - val_dice_loss: 0.2839 - val_iou_coeff: 0.6141 - val_precision: 0.7728 - val_recall: 0.6381\n\nEpoch 00021: val_dice_loss did not improve from 0.26583\nEpoch 22/40\n1912/1912 [==============================] - 618s 323ms/step - loss: 0.2133 - ACL5: 2388.3275 - binary_crossentropy_plus_jaccard_loss: 0.2133 - dice_coef: 0.8858 - dsc: 0.8887 - dice_loss: 0.1113 - iou_coeff: 0.8028 - precision: 0.8922 - recall: 0.8888 - val_loss: 0.4896 - val_ACL5: 2130.8486 - val_binary_crossentropy_plus_jaccard_loss: 0.4896 - val_dice_coef: 0.6903 - val_dsc: 0.6903 - val_dice_loss: 0.3097 - val_iou_coeff: 0.5929 - val_precision: 0.7327 - val_recall: 0.6370\n\nEpoch 00022: val_dice_loss did not improve from 0.26583\nEpoch 23/40\n1912/1912 [==============================] - 625s 327ms/step - loss: 0.2114 - ACL5: 2340.2642 - binary_crossentropy_plus_jaccard_loss: 0.2114 - dice_coef: 0.8890 - dsc: 0.8897 - dice_loss: 0.1103 - iou_coeff: 0.8048 - precision: 0.8934 - recall: 0.8888 - val_loss: 0.4793 - val_ACL5: 2016.8610 - val_binary_crossentropy_plus_jaccard_loss: 0.4793 - val_dice_coef: 0.7060 - val_dsc: 0.7060 - val_dice_loss: 0.2940 - val_iou_coeff: 0.6070 - val_precision: 0.7534 - val_recall: 0.6243\n\nEpoch 00023: val_dice_loss did not improve from 0.26583\nEpoch 24/40\n1912/1912 [==============================] - 624s 326ms/step - loss: 0.2097 - ACL5: 2346.2815 - binary_crossentropy_plus_jaccard_loss: 0.2097 - dice_coef: 0.8894 - dsc: 0.8908 - dice_loss: 0.1092 - iou_coeff: 0.8066 - precision: 0.8933 - recall: 0.8906 - val_loss: 0.4778 - val_ACL5: 2045.0323 - val_binary_crossentropy_plus_jaccard_loss: 0.4778 - val_dice_coef: 0.7177 - val_dsc: 0.7177 - val_dice_loss: 0.2823 - val_iou_coeff: 0.6177 - val_precision: 0.7556 - val_recall: 0.6418\n\nEpoch 00024: val_dice_loss did not improve from 0.26583\nEpoch 25/40\n1912/1912 [==============================] - 627s 328ms/step - loss: 0.2050 - ACL5: 2277.9440 - binary_crossentropy_plus_jaccard_loss: 0.2050 - dice_coef: 0.8921 - dsc: 0.8937 - dice_loss: 0.1063 - iou_coeff: 0.8108 - precision: 0.8960 - recall: 0.8932 - val_loss: 0.5147 - val_ACL5: 2145.4705 - val_binary_crossentropy_plus_jaccard_loss: 0.5147 - val_dice_coef: 0.6751 - val_dsc: 0.6751 - val_dice_loss: 0.3249 - val_iou_coeff: 0.5785 - val_precision: 0.7391 - val_recall: 0.5881\n\nEpoch 00025: val_dice_loss did not improve from 0.26583\nEpoch 26/40\n1912/1912 [==============================] - 627s 328ms/step - loss: 0.2025 - ACL5: 2278.7472 - binary_crossentropy_plus_jaccard_loss: 0.2025 - dice_coef: 0.8917 - dsc: 0.8947 - dice_loss: 0.1053 - iou_coeff: 0.8125 - precision: 0.8988 - recall: 0.8945 - val_loss: 0.4889 - val_ACL5: 2077.8032 - val_binary_crossentropy_plus_jaccard_loss: 0.4889 - val_dice_coef: 0.6920 - val_dsc: 0.6920 - val_dice_loss: 0.3080 - val_iou_coeff: 0.5955 - val_precision: 0.7336 - val_recall: 0.6197\n\nEpoch 00026: val_dice_loss did not improve from 0.26583\nEpoch 27/40\n1912/1912 [==============================] - 631s 330ms/step - loss: 0.1997 - ACL5: 2225.6788 - binary_crossentropy_plus_jaccard_loss: 0.1997 - dice_coef: 0.8931 - dsc: 0.8960 - dice_loss: 0.1040 - iou_coeff: 0.8141 - precision: 0.9012 - recall: 0.8961 - val_loss: 0.4854 - val_ACL5: 2050.6643 - val_binary_crossentropy_plus_jaccard_loss: 0.4854 - val_dice_coef: 0.6964 - val_dsc: 0.6964 - val_dice_loss: 0.3036 - val_iou_coeff: 0.5995 - val_precision: 0.7599 - val_recall: 0.6218\n\nEpoch 00027: val_dice_loss did not improve from 0.26583\nEpoch 28/40\n1912/1912 [==============================] - 634s 332ms/step - loss: 0.1945 - ACL5: 2183.5759 - binary_crossentropy_plus_jaccard_loss: 0.1945 - dice_coef: 0.8994 - dsc: 0.9005 - dice_loss: 0.0995 - iou_coeff: 0.8217 - precision: 0.9021 - recall: 0.8980 - val_loss: 0.5526 - val_ACL5: 2276.1423 - val_binary_crossentropy_plus_jaccard_loss: 0.5526 - val_dice_coef: 0.6458 - val_dsc: 0.6458 - val_dice_loss: 0.3542 - val_iou_coeff: 0.5540 - val_precision: 0.7114 - val_recall: 0.5743\n\nEpoch 00028: val_dice_loss did not improve from 0.26583\nEpoch 29/40\n1912/1912 [==============================] - 632s 330ms/step - loss: 0.1944 - ACL5: 2212.7094 - binary_crossentropy_plus_jaccard_loss: 0.1944 - dice_coef: 0.8946 - dsc: 0.8990 - dice_loss: 0.1010 - iou_coeff: 0.8190 - precision: 0.9039 - recall: 0.8999 - val_loss: 0.5387 - val_ACL5: 2189.5857 - val_binary_crossentropy_plus_jaccard_loss: 0.5387 - val_dice_coef: 0.6646 - val_dsc: 0.6646 - val_dice_loss: 0.3354 - val_iou_coeff: 0.5695 - val_precision: 0.7237 - val_recall: 0.5692\n\nEpoch 00029: val_dice_loss did not improve from 0.26583\nEpoch 30/40\n1912/1912 [==============================] - 632s 330ms/step - loss: 0.1903 - ACL5: 2130.1630 - binary_crossentropy_plus_jaccard_loss: 0.1903 - dice_coef: 0.9005 - dsc: 0.9026 - dice_loss: 0.0974 - iou_coeff: 0.8248 - precision: 0.9044 - recall: 0.9014 - val_loss: 0.5133 - val_ACL5: 2172.9395 - val_binary_crossentropy_plus_jaccard_loss: 0.5133 - val_dice_coef: 0.6796 - val_dsc: 0.6796 - val_dice_loss: 0.3204 - val_iou_coeff: 0.5835 - val_precision: 0.7145 - val_recall: 0.6097\n\nEpoch 00030: val_dice_loss did not improve from 0.26583\nEpoch 31/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1893 - ACL5: 2113.8359 - binary_crossentropy_plus_jaccard_loss: 0.1893 - dice_coef: 0.9041 - dsc: 0.9043 - dice_loss: 0.0957 - iou_coeff: 0.8281 - precision: 0.9031 - recall: 0.9010 - val_loss: 0.5076 - val_ACL5: 2118.0681 - val_binary_crossentropy_plus_jaccard_loss: 0.5076 - val_dice_coef: 0.6760 - val_dsc: 0.6760 - val_dice_loss: 0.3240 - val_iou_coeff: 0.5802 - val_precision: 0.7467 - val_recall: 0.6012\n\nEpoch 00031: val_dice_loss did not improve from 0.26583\nEpoch 32/40\n1912/1912 [==============================] - 634s 332ms/step - loss: 0.1829 - ACL5: 2086.2946 - binary_crossentropy_plus_jaccard_loss: 0.1829 - dice_coef: 0.9056 - dsc: 0.9065 - dice_loss: 0.0935 - iou_coeff: 0.8311 - precision: 0.9092 - recall: 0.9055 - val_loss: 0.5040 - val_ACL5: 2140.7727 - val_binary_crossentropy_plus_jaccard_loss: 0.5040 - val_dice_coef: 0.6907 - val_dsc: 0.6907 - val_dice_loss: 0.3093 - val_iou_coeff: 0.5927 - val_precision: 0.7329 - val_recall: 0.6130\n\nEpoch 00032: val_dice_loss did not improve from 0.26583\nEpoch 33/40\n1912/1912 [==============================] - 634s 331ms/step - loss: 0.1808 - ACL5: 2039.9326 - binary_crossentropy_plus_jaccard_loss: 0.1808 - dice_coef: 0.9061 - dsc: 0.9077 - dice_loss: 0.0923 - iou_coeff: 0.8335 - precision: 0.9104 - recall: 0.9051 - val_loss: 0.4951 - val_ACL5: 2166.2085 - val_binary_crossentropy_plus_jaccard_loss: 0.4951 - val_dice_coef: 0.6794 - val_dsc: 0.6794 - val_dice_loss: 0.3206 - val_iou_coeff: 0.5800 - val_precision: 0.7676 - val_recall: 0.6218\n\nEpoch 00033: val_dice_loss did not improve from 0.26583\nEpoch 34/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1825 - ACL5: 2061.0035 - binary_crossentropy_plus_jaccard_loss: 0.1825 - dice_coef: 0.9065 - dsc: 0.9079 - dice_loss: 0.0921 - iou_coeff: 0.8340 - precision: 0.9079 - recall: 0.9041 - val_loss: 0.7509 - val_ACL5: 2901.0569 - val_binary_crossentropy_plus_jaccard_loss: 0.7509 - val_dice_coef: 0.4982 - val_dsc: 0.4982 - val_dice_loss: 0.5018 - val_iou_coeff: 0.4247 - val_precision: 0.6040 - val_recall: 0.3843\n\nEpoch 00034: val_dice_loss did not improve from 0.26583\nEpoch 35/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1817 - ACL5: 2072.5251 - binary_crossentropy_plus_jaccard_loss: 0.1817 - dice_coef: 0.9065 - dsc: 0.9066 - dice_loss: 0.0934 - iou_coeff: 0.8324 - precision: 0.9097 - recall: 0.9046 - val_loss: 0.5386 - val_ACL5: 2137.6436 - val_binary_crossentropy_plus_jaccard_loss: 0.5386 - val_dice_coef: 0.6597 - val_dsc: 0.6597 - val_dice_loss: 0.3403 - val_iou_coeff: 0.5702 - val_precision: 0.7100 - val_recall: 0.5784\n\nEpoch 00035: val_dice_loss did not improve from 0.26583\nEpoch 36/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1792 - ACL5: 2037.1042 - binary_crossentropy_plus_jaccard_loss: 0.1792 - dice_coef: 0.9095 - dsc: 0.9092 - dice_loss: 0.0908 - iou_coeff: 0.8365 - precision: 0.9095 - recall: 0.9061 - val_loss: 0.5067 - val_ACL5: 2091.5474 - val_binary_crossentropy_plus_jaccard_loss: 0.5067 - val_dice_coef: 0.6818 - val_dsc: 0.6818 - val_dice_loss: 0.3182 - val_iou_coeff: 0.5838 - val_precision: 0.7684 - val_recall: 0.6051\n\nEpoch 00036: val_dice_loss did not improve from 0.26583\nEpoch 37/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1725 - ACL5: 1971.1110 - binary_crossentropy_plus_jaccard_loss: 0.1725 - dice_coef: 0.9125 - dsc: 0.9129 - dice_loss: 0.0871 - iou_coeff: 0.8420 - precision: 0.9135 - recall: 0.9104 - val_loss: 0.5194 - val_ACL5: 2175.7537 - val_binary_crossentropy_plus_jaccard_loss: 0.5194 - val_dice_coef: 0.6803 - val_dsc: 0.6803 - val_dice_loss: 0.3197 - val_iou_coeff: 0.5868 - val_precision: 0.7509 - val_recall: 0.5964\n\nEpoch 00037: val_dice_loss did not improve from 0.26583\nEpoch 38/40\n1912/1912 [==============================] - 632s 331ms/step - loss: 0.1725 - ACL5: 1989.2375 - binary_crossentropy_plus_jaccard_loss: 0.1725 - dice_coef: 0.9121 - dsc: 0.9121 - dice_loss: 0.0879 - iou_coeff: 0.8414 - precision: 0.9141 - recall: 0.9098 - val_loss: 0.6457 - val_ACL5: 2632.3816 - val_binary_crossentropy_plus_jaccard_loss: 0.6457 - val_dice_coef: 0.5741 - val_dsc: 0.5741 - val_dice_loss: 0.4259 - val_iou_coeff: 0.4866 - val_precision: 0.7148 - val_recall: 0.4753\n\nEpoch 00038: val_dice_loss did not improve from 0.26583\nEpoch 39/40\n1912/1912 [==============================] - 632s 331ms/step - loss: 0.1717 - ACL5: 1952.5827 - binary_crossentropy_plus_jaccard_loss: 0.1717 - dice_coef: 0.9123 - dsc: 0.9141 - dice_loss: 0.0859 - iou_coeff: 0.8444 - precision: 0.9140 - recall: 0.9091 - val_loss: 0.5173 - val_ACL5: 2063.2383 - val_binary_crossentropy_plus_jaccard_loss: 0.5173 - val_dice_coef: 0.6843 - val_dsc: 0.6843 - val_dice_loss: 0.3157 - val_iou_coeff: 0.5934 - val_precision: 0.7243 - val_recall: 0.6014\n\nEpoch 00039: val_dice_loss did not improve from 0.26583\nEpoch 40/40\n1912/1912 [==============================] - 633s 331ms/step - loss: 0.1683 - ACL5: 1940.8746 - binary_crossentropy_plus_jaccard_loss: 0.1683 - dice_coef: 0.9143 - dsc: 0.9151 - dice_loss: 0.0849 - iou_coeff: 0.8461 - precision: 0.9153 - recall: 0.9126 - val_loss: 0.5334 - val_ACL5: 2210.3191 - val_binary_crossentropy_plus_jaccard_loss: 0.5334 - val_dice_coef: 0.6666 - val_dsc: 0.6666 - val_dice_loss: 0.3334 - val_iou_coeff: 0.5737 - val_precision: 0.7247 - val_recall: 0.5916\n\nEpoch 00040: val_dice_loss did not improve from 0.26583\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}